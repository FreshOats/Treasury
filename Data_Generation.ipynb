{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Synthetic Data for Treasury Analytics\n",
    "\n",
    "1. Transactions Table (Largest Dataset, 10000 records)\n",
    "Reasoning: Financial transactions typically occur frequently and continuously, resulting in a large volume of records. This dataset would naturally be the largest, as it captures daily operational expenses, revenues, vendor payments, and other financial activities. Transactional data often accumulates rapidly, making it the most extensive dataset.\n",
    "\n",
    "2. Investment Portfolio Holdings (Second Largest, 250 records)\n",
    "Reasoning: Investment portfolios usually contain numerous securities, each with multiple transactions (buys, sells, dividends, interest payments). While not as frequent as daily operational transactions, investment holdings still generate substantial data, especially if historical records and periodic valuations are included.\n",
    "\n",
    "3. Unclaimed Property Records (Third Largest, 200 records)\n",
    "Reasoning: Unclaimed property records accumulate steadily but at a slower rate compared to financial transactions or investment activities. These records are typically updated periodically (monthly, quarterly, or annually), resulting in fewer records than the first two datasets.\n",
    "\n",
    "4. Program Performance Metrics (Smallest Dataset, 20-50 records)\n",
    "Reasoning: Program performance metrics are usually aggregated data points collected periodically (monthly, quarterly, or annually). They summarize program outcomes, participant counts, and costs, resulting in fewer records compared to transactional or investment data.\n",
    "Recommended Dataset Sizes (for your dummy data project):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from faker import Faker\n",
    "import random\n",
    "\n",
    "fake = Faker()\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(91942)\n",
    "random_state = random.seed(91942)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Dataset 1: ISTO Financial Transactions\n",
    "Purpose: Analyze spending patterns, identify cost-saving opportunities, and visualize financial trends.\n",
    "\n",
    "**Schema:**\n",
    "- Transaction_ID (unique identifier) VARCHAR\n",
    "- Transaction_Date DATETIME\n",
    "- Transaction_Amount FLOAT\n",
    "- Transaction_Type (Expense, Revenue) VARCHAR\n",
    "- Department (e.g., Operations, Investments, Community Programs) VARCHAR\n",
    "- Vendor_Name VARCHAR\n",
    "- Description VARCHAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Transaction_ID Transaction_Date  Transaction_Amount Transaction_Type  \\\n",
      "0    TXN00000001       2023-06-13            52895.59          Expense   \n",
      "1    TXN00000002       2024-12-21            99357.18          Expense   \n",
      "2    TXN00000003       2024-12-29            27593.75          Expense   \n",
      "3    TXN00000004       2025-02-21           105942.45          Revenue   \n",
      "4    TXN00000005       2023-12-25            77628.58          Expense   \n",
      "\n",
      "           Department                Vendor_Name        Description  \n",
      "0          Operations             Gonzalez-Hines        Maintenance  \n",
      "1                  IT                Bennett LLC  Event Sponsorship  \n",
      "2      Administration              Jackson-Smith    Travel Expenses  \n",
      "3  Community Programs             Hardy and Sons      Grant Funding  \n",
      "4  Community Programs  Porter, Ramirez and Jones    Office Supplies  \n",
      "Total transactions: 10000\n",
      "Unique vendors: 835\n",
      "Vendors appearing more than once: 504\n"
     ]
    }
   ],
   "source": [
    "# Number of rows\n",
    "num_rows = 10000\n",
    "num_unique_vendors = 1159 \n",
    "\n",
    "# Generate Transaction IDs (formatted as TXN00000001)\n",
    "transaction_ids = [f'TXN{str(i).zfill(8)}' for i in range(1, num_rows + 1)]\n",
    "\n",
    "# Generate random dates within a realistic range (e.g., past 2 years)\n",
    "transaction_dates = [fake.date_between(start_date='-2y', end_date='today') for _ in range(num_rows)]\n",
    "\n",
    "# Generate transaction amounts (expenses and revenues)\n",
    "transaction_amounts = np.round(np.random.uniform(50, 150000, num_rows), 2)\n",
    "\n",
    "# Randomly assign transaction types\n",
    "transaction_types = np.random.choice(['Expense', 'Revenue'], size=num_rows, p=[0.7, 0.3])\n",
    "\n",
    "# Departments\n",
    "departments = ['Operations', 'Investments', 'Community Programs', 'Administration', 'IT', 'Compliance']\n",
    "department_choices = np.random.choice(departments, size=num_rows)\n",
    "\n",
    "# Vendor names (using Faker), limiting to lower unique vendors\n",
    "unique_vendors = [fake.unique.company() for _ in range(num_unique_vendors)]\n",
    "\n",
    "# Create a realistic distribution of vendor frequencies\n",
    "vendor_probabilities = np.random.zipf(a=2, size=num_unique_vendors)\n",
    "vendor_probabilities = vendor_probabilities / vendor_probabilities.sum()\n",
    "\n",
    "# Assign vendors to transactions based on probabilities\n",
    "vendor_names = np.random.choice(unique_vendors, size=num_rows, p=vendor_probabilities)\n",
    "\n",
    "# Transaction descriptions (simple examples)\n",
    "descriptions_expense = ['Office Supplies', 'Consulting Fees', 'Software Subscription', 'Event Sponsorship', 'Maintenance', 'Travel Expenses']\n",
    "descriptions_revenue = ['Interest Income', 'Investment Returns', 'Program Fees', 'Grant Funding', 'Reimbursement']\n",
    "\n",
    "descriptions = [\n",
    "    random.choice(descriptions_expense) if t == 'Expense' else random.choice(descriptions_revenue)\n",
    "    for t in transaction_types\n",
    "]\n",
    "\n",
    "# Assemble DataFrame\n",
    "transactions_df = pd.DataFrame({\n",
    "    'Transaction_ID': transaction_ids,\n",
    "    'Transaction_Date': transaction_dates,\n",
    "    'Transaction_Amount': transaction_amounts,\n",
    "    'Transaction_Type': transaction_types,\n",
    "    'Department': department_choices,\n",
    "    'Vendor_Name': vendor_names,\n",
    "    'Description': descriptions\n",
    "})\n",
    "\n",
    "# Preview the dataset\n",
    "print(transactions_df.head())\n",
    "print(\"Total transactions:\", len(transactions_df))\n",
    "print(\"Unique vendors:\", transactions_df.Vendor_Name.nunique())\n",
    "print(\"Vendors appearing more than once:\", (transactions_df.Vendor_Name.value_counts() > 1).sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has created a perfect dataframe with no errors or duplicates. This is unrealistic, so let's shake it up.\n",
    "\n",
    "1. Introduce dupes\n",
    "2. Add incorrect data types\n",
    "3. Add logical inconsistencies\n",
    "4. Give it some outliers!\n",
    "5. Mess up the vendor names including INC\n",
    "6. Remove a few IDs\n",
    "7. Spread some NaN love throughout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Transaction_ID Transaction_Date  Transaction_Amount Transaction_Type  \\\n",
      "0    TXN00001968       2024-04-16           109876.08          Expense   \n",
      "1    TXN00008799       2023-06-06            47425.61          Expense   \n",
      "2    TXN00008096       2024-10-30            95756.35          Expense   \n",
      "3    TXN00004666       2023-07-14            59383.96          Expense   \n",
      "4    TXN00005871       2024-09-05            28682.69          Expense   \n",
      "\n",
      "       Department     Vendor_Name            Description  \n",
      "0              IT  Hardy and Sons      Event Sponsorship  \n",
      "1  Administration  Summers-Arroyo        Travel Expenses  \n",
      "2             NaN     Diaz-Nguyen        Consulting Fees  \n",
      "3  Administration  Hardy and Sons        Office Supplies  \n",
      "4     Investments  Hardy and Sons  Software Subscription  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\justi\\AppData\\Local\\Temp\\ipykernel_30972\\1005332821.py:16: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['134802.87' '81216.53' '18307.27' '7322.77' '46476.1' '39314.65'\n",
      " '24768.73' '127700.63' '146614.62' '43446.66' '92902.67' '-77043.93'\n",
      " '50876.35' '37490.58' '117194.16' '71490.68' '9988.16' '95227.11'\n",
      " '18581.42' '64127.11' '51566.91' '143417.13' '139726.63' '31963.7' 'nan'\n",
      " '61560.13' '143283.57' '115795.73' '1195773.053480141' '56723.94'\n",
      " '29745.52' '20653.11' '146142.95' '101.64' '146276.02' '65322.12'\n",
      " '61984.06' '104262.34' '6252.21' '127733.11' '-134970.86' '65192.2'\n",
      " '10436.19' '119566.75' '74805.43' '123384.1' '84547.95' '48686.1'\n",
      " '130846.52' '4692.63' '112368.66' '12834.14387981832' '137825.92'\n",
      " '48037.54' '112252.99' '70064.19' '73397.57' '29256.43' '117786.37'\n",
      " '260.13220444691933' '17772.8' '1163.659649618186' '314562.00130865327'\n",
      " '-4641.92' '61022.99' '128378.34' '122450.74' '80255.94' '12447.93'\n",
      " '78236.95' '44588.09' '1629.25' '49257.45' '140118.11' '139233.47'\n",
      " '2324.71' '135714.42' '144697.35' '67833.13' '144558.88' '50519.69'\n",
      " '132291.67' '148771.13' '36845.9546855062' '126765.86' '8438.69'\n",
      " '90345.11' '17979.27' '102085.75' '96023.77' '94949.45' '55160.46'\n",
      " '119617.79' '89347.05' '85518.09' '88754.06' '68603.47' '85537.78'\n",
      " '38600.2' '37787.8' '277.56' '672.83' 'nan' 'nan' '134017.07' '101348.03'\n",
      " '36927.54' '100749.8' '95683.71' '82216.4' '99471.14' '14724.21' 'nan'\n",
      " '108925.74' '44811.79' '15288.53' 'nan' '42830.31' '145838.79' '81341.99'\n",
      " '94078.65' '129672.01' '49578.95' '105690.19' '13250.42' '114510.22'\n",
      " '45510.47' '87925.55' '19389.15' '80823.0' '140316.07' '136387.53'\n",
      " '63742.98' '33721.66' '71527.72' '54613.53' '89431.08' '133755.3'\n",
      " '52529.24' '90755.17' '99888.37' '47492.82' '1816.71' '27609.59'\n",
      " '114508.57' '73054.27' '29382.94' '26432.6' '80529.06' '24943.19'\n",
      " '75573.22' '24695.04' '50587.2' '20198.75' '82278.52' '115062.44'\n",
      " '140078.64' '101463.59' '43784.45' '77372.52' '101079.96' '15328.08'\n",
      " '86619.41' '49773.14' '142599.32' '34509.75' '122524.69' '146843.44'\n",
      " '146505.22' '55329.17' '86305.52' '65136.18' '79845.0' '123279.14'\n",
      " '44157.10312154222' '134033.67' 'nan' '146766.12' '4697.47' '15099.77'\n",
      " '60235.95' '103176.36' '92691.71' '113166.86' '17409.46'\n",
      " '14785.955240432479' '56849.01' '15758.4' '22659.7' '60730.77' '96444.57'\n",
      " '47172.53' '49253.52' '96023.69' '21591.99' '78355.18' '97010.77'\n",
      " '89362.3' '53805.36' '33770.61' '142311.16' '72791.28' '4881.71'\n",
      " '95221.35' '93357.09' '37233.01' '23718.33' '33459.6']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  transactions_df.loc[type_error_indices, 'Transaction_Amount'] = transactions_df.loc[type_error_indices, 'Transaction_Amount'].astype(str)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(91942)\n",
    "random_state = random.seed(91942)\n",
    "random_state_alt = random.seed(24919)\n",
    "\n",
    "# 1. Introduce duplicate rows\n",
    "num_duplicates = int(len(transactions_df) * 0.02)  # 2% duplicates\n",
    "duplicates_df = transactions_df.sample(n=num_duplicates, random_state=random_state)\n",
    "transactions_df = pd.concat([transactions_df, duplicates_df], ignore_index=True)\n",
    "\n",
    "# 2. Introduce incorrect data types\n",
    "num_type_errors = int(len(transactions_df) * 0.02)  # 2% type errors\n",
    "type_error_indices = transactions_df.sample(n=num_type_errors, random_state=random_state).index\n",
    "transactions_df.loc[type_error_indices, 'Transaction_Amount'] = transactions_df.loc[type_error_indices, 'Transaction_Amount'].astype(str)\n",
    "\n",
    "date_error_indices = transactions_df.sample(n=num_type_errors, random_state=random_state_alt).index\n",
    "transactions_df.loc[date_error_indices, 'Transaction_Date'] = transactions_df.loc[date_error_indices, 'Transaction_Date'].astype(str) + ' INVALID'\n",
    "\n",
    "# 3. Introduce logical inconsistencies\n",
    "expense_indices = transactions_df[transactions_df['Transaction_Type'] == 'Expense'].sample(n=num_type_errors, random_state=random_state).index\n",
    "transactions_df.loc[expense_indices, 'Transaction_Amount'] *= -1  # Negative amounts for expenses\n",
    "\n",
    "invalid_department_indices = transactions_df.sample(n=num_type_errors, random_state=random_state_alt).index\n",
    "transactions_df.loc[invalid_department_indices, 'Department'] = 'InvalidDept'\n",
    "\n",
    "# 4. Introduce outliers\n",
    "# Ensure 'Transaction_Amount' is numeric\n",
    "transactions_df['Transaction_Amount'] = pd.to_numeric(transactions_df['Transaction_Amount'], errors='coerce')\n",
    "outlier_indices = transactions_df.sample(n=num_type_errors, random_state=random_state).index\n",
    "\n",
    "# Generate outliers using a log-normal distribution\n",
    "outlier_values = np.random.lognormal(mean=10, sigma=2, size=num_type_errors)\n",
    "transactions_df.loc[outlier_indices, 'Transaction_Amount'] = outlier_values\n",
    "\n",
    "future_date_indices = transactions_df.sample(n=num_type_errors, random_state=random_state_alt).index\n",
    "transactions_df.loc[future_date_indices, 'Transaction_Date'] = pd.Timestamp('today') + pd.to_timedelta(np.random.randint(30, 365, size=num_type_errors), unit='d')\n",
    "\n",
    "# 5. Introduce inconsistent vendor names\n",
    "vendor_indices = transactions_df.sample(n=num_type_errors, random_state=random_state).index\n",
    "transactions_df.loc[vendor_indices, 'Vendor_Name'] = transactions_df.loc[vendor_indices, 'Vendor_Name'].apply(\n",
    "    lambda x: x.replace('Inc.', 'Incorporated') if 'Inc.' in x else x + ' Inc.'\n",
    ")\n",
    "\n",
    "# 6. Introduce missing or invalid Transaction IDs\n",
    "missing_id_indices = transactions_df.sample(n=num_type_errors, random_state=random_state).index\n",
    "transactions_df.loc[missing_id_indices, 'Transaction_ID'] = np.nan\n",
    "\n",
    "# 7. Introduce missing values (nulls) - needs to be last so it doesn't break the multiplication code\n",
    "num_nulls = int(len(transactions_df) * 0.03)  # 3% nulls\n",
    "null_indices_amount = transactions_df.sample(n=num_nulls, random_state=random_state).index\n",
    "null_indices_department = transactions_df.sample(n=num_nulls, random_state=random_state_alt).index\n",
    "transactions_df.loc[null_indices_amount, 'Transaction_Amount'] = np.nan\n",
    "transactions_df.loc[null_indices_department, 'Department'] = np.nan\n",
    "\n",
    "# Shuffle the DataFrame to mix errors naturally\n",
    "transactions_df = transactions_df.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "\n",
    "# Check the resulting DataFrame\n",
    "print(transactions_df.head())\n",
    "# print(transactions_df.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique vendors: 939\n",
      "Number of duplicated vendors: 9465\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of unique vendors:\", len(transactions_df.Vendor_Name.unique()))\n",
    "print(\"Number of duplicated vendors:\", transactions_df.Vendor_Name.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, save the data in a horrible data format: csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV and cringe\n",
    "transactions_df.to_csv('transactions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Dataset #2 Investment Portfolio Holdings\n",
    "Purpose: Analyze ISTO's investment portfolio, diversification, and risk management.\n",
    "\n",
    "**Schema:**\n",
    "- Holding_ID VARCHAR\n",
    "- Security_Name VARCHAR\n",
    "- Security_Type VARCHAR\n",
    "- Quantity_Held INT\n",
    "- Market_Value FLOAT\n",
    "- Acquisition_Date DATETIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Holding_ID            Security_Name Security_Type  Quantity_Held  \\\n",
      "0   HLD00001   Diaz, Walker and Cline         Stock            544   \n",
      "1   HLD00002         Johnson and Sons          Bond           1106   \n",
      "2   HLD00003  Frazier, Shaw and Davis          Bond           1841   \n",
      "3   HLD00004              Edwards Ltd   Mutual Fund           1427   \n",
      "4   HLD00005          Valencia-Miller         Stock            805   \n",
      "\n",
      "   Market_Price_Per_Unit  Total_Market_Value Acquisition_Date  \n",
      "0                 493.30           268355.20       2021-08-11  \n",
      "1                  44.65            49382.90       2025-03-07  \n",
      "2                  53.83            99101.03       2023-03-08  \n",
      "3                 424.49           605747.23       2022-10-12  \n",
      "4                  96.97            78060.85       2025-02-06  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from faker import Faker\n",
    "\n",
    "fake = Faker()\n",
    "np.random.seed(91942)\n",
    "\n",
    "# Number of holdings\n",
    "num_holdings = 257\n",
    "\n",
    "# Generate Holding IDs\n",
    "holding_ids = [f'HLD{str(i).zfill(5)}' for i in range(1, num_holdings + 1)]\n",
    "\n",
    "# Generate realistic security names (e.g., company names)\n",
    "security_names = [fake.unique.company() for _ in range(num_holdings)]\n",
    "\n",
    "# Security types distribution\n",
    "security_types = np.random.choice(\n",
    "    ['Stock', 'Bond', 'ETF', 'Mutual Fund'],\n",
    "    size=num_holdings,\n",
    "    p=[0.5, 0.2, 0.2, 0.1]\n",
    ")\n",
    "\n",
    "# Quantity held (realistic quantities)\n",
    "quantity_held = np.random.randint(100, 5000, size=num_holdings)\n",
    "\n",
    "# Market value per unit (realistic prices)\n",
    "market_price_per_unit = np.round(np.random.uniform(10, 500, size=num_holdings), 2)\n",
    "\n",
    "# Calculate total market value\n",
    "market_values = np.round(quantity_held * market_price_per_unit, 2)\n",
    "\n",
    "# Acquisition dates (within past 5 years)\n",
    "acquisition_dates = pd.to_datetime([fake.date_between(start_date='-5y', end_date='today') for _ in range(num_holdings)])\n",
    "\n",
    "\n",
    "# Assemble DataFrame\n",
    "portfolio_holdings_df = pd.DataFrame({\n",
    "    'Holding_ID': holding_ids,\n",
    "    'Security_Name': security_names,\n",
    "    'Security_Type': security_types,\n",
    "    'Quantity_Held': quantity_held,\n",
    "    'Market_Price_Per_Unit': market_price_per_unit,\n",
    "    'Total_Market_Value': market_values,\n",
    "    'Acquisition_Date': acquisition_dates\n",
    "})\n",
    "\n",
    "# Preview the dataset\n",
    "print(portfolio_holdings_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a perfect dataset, let's make it crappy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Holding_ID             Security_Name Security_Type  Quantity_Held  \\\n",
      "0   HLD00144               Jackson Inc         Stock           2908   \n",
      "1   HLD00145          Sandoval-Herrera           ETF           2357   \n",
      "2   HLD00226                Carter PLC         Stock           2428   \n",
      "3   HLD00182              Gomez-Harris   Mutual Fund           4352   \n",
      "4   HLD00239  Owens, Miller and Gibson          Bond           4716   \n",
      "\n",
      "   Market_Price_Per_Unit  Total_Market_Value Acquisition_Date  \n",
      "0                 157.64           458417.12       2021-01-09  \n",
      "1                 487.14          1148188.98       2022-07-04  \n",
      "2                 201.93           490286.04       2024-11-19  \n",
      "3                 492.02          2141271.04       2021-01-23  \n",
      "4                 426.09          2009440.44       2021-11-05  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from faker import Faker\n",
    "\n",
    "fake = Faker()\n",
    "np.random.seed(91942)\n",
    "random_state=random.seed(91942)\n",
    "random_state_alt = random.seed(24919)\n",
    "\n",
    "# 1. Introduce duplicate holdings with slight variations (3%)\n",
    "num_duplicates = int(len(portfolio_holdings_df) * 0.03)\n",
    "duplicates_df = portfolio_holdings_df.sample(n=num_duplicates, random_state=random_state).copy()\n",
    "\n",
    "duplicates_df['Security_Name'] = duplicates_df['Security_Name'].apply(\n",
    "    lambda x: x + ' Inc.' if 'Inc.' not in x else x.replace('Inc.', 'Incorporated')\n",
    ")\n",
    "duplicates_df['Acquisition_Date'] += pd.to_timedelta(\n",
    "    np.random.randint(-10, 10, size=num_duplicates), unit='d'\n",
    ")\n",
    "\n",
    "# 2. Introduce missing values in 'Market_Price_Per_Unit' (2%)\n",
    "num_nulls = int(len(portfolio_holdings_df) * 0.02)\n",
    "null_indices = portfolio_holdings_df.sample(n=num_nulls, random_state=random_state).index\n",
    "portfolio_holdings_df.loc[null_indices, 'Market_Price_Per_Unit'] = np.nan\n",
    "\n",
    "# 3. Introduce unrealistic market prices (outliers) (2%)\n",
    "num_outliers = int(len(portfolio_holdings_df) * 0.02)\n",
    "outlier_indices = portfolio_holdings_df.sample(n=num_outliers, random_state=random_state_alt).index\n",
    "portfolio_holdings_df.loc[outlier_indices, 'Market_Price_Per_Unit'] *= np.random.choice([0.01, 100], size=num_outliers)\n",
    "portfolio_holdings_df['Total_Market_Value'] = portfolio_holdings_df['Quantity_Held'] * portfolio_holdings_df['Market_Price_Per_Unit']\n",
    "\n",
    "# 4. Introduce incorrect security types (misclassification) (3%)\n",
    "num_misclassified = int(len(portfolio_holdings_df) * 0.03)\n",
    "misclassified_indices = portfolio_holdings_df.sample(n=num_misclassified, random_state=random_state).index\n",
    "portfolio_holdings_df.loc[misclassified_indices, 'Security_Type'] = np.random.choice(['Stock', 'Bond', 'ETF', 'Mutual Fund'], size=num_misclassified)\n",
    "\n",
    "# 5. Introduce future acquisition dates (2%)\n",
    "num_future_dates = int(len(portfolio_holdings_df) * 0.02)\n",
    "future_indices = portfolio_holdings_df.sample(n=num_future_dates, random_state=random_state_alt).index\n",
    "portfolio_holdings_df.loc[future_indices, 'Acquisition_Date'] = pd.Timestamp('today') + pd.to_timedelta(np.random.randint(30, 365, size=num_future_dates), unit='d')\n",
    "\n",
    "# Combine duplicates back into the original DataFrame\n",
    "portfolio_holdings_df = pd.concat([portfolio_holdings_df, duplicates_df], ignore_index=True)\n",
    "\n",
    "# Shuffle the DataFrame to mix errors naturally\n",
    "portfolio_holdings_df = portfolio_holdings_df.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "\n",
    "# Check the resulting DataFrame\n",
    "print(portfolio_holdings_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV and cringe some more\n",
    "portfolio_holdings_df.to_csv('portfolio_holdings.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Dataset #3: "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "treasury",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
