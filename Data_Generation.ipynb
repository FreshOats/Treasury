{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Synthetic Data for Treasury Analytics\n",
    "\n",
    "1. Transactions Table (Largest Dataset, 10000 records)\n",
    "Reasoning: Financial transactions typically occur frequently and continuously, resulting in a large volume of records. This dataset would naturally be the largest, as it captures daily operational expenses, revenues, vendor payments, and other financial activities. Transactional data often accumulates rapidly, making it the most extensive dataset.\n",
    "\n",
    "2. Investment Portfolio Holdings (Second Largest, 250 records)\n",
    "Reasoning: Investment portfolios usually contain numerous securities, each with multiple transactions (buys, sells, dividends, interest payments). While not as frequent as daily operational transactions, investment holdings still generate substantial data, especially if historical records and periodic valuations are included.\n",
    "\n",
    "3. External Vendor Information (~200 records)\n",
    "Reasoning: This dimension table focuses on vendors with formal contracts or significant financial relationships with the organization.\n",
    "\n",
    "\n",
    "4. Unclaimed Property Records (~200 records)\n",
    "Reasoning: Unclaimed property records accumulate steadily but at a slower rate compared to financial transactions or investment activities. These records are typically updated periodically (monthly, quarterly, or annually), resulting in fewer records than the first two datasets.\n",
    "\n",
    "5. Program Performance Metrics (Smallest Dataset, 20-50 records)\n",
    "Reasoning: Program performance metrics are usually aggregated data points collected periodically (monthly, quarterly, or annually). They summarize program outcomes, participant counts, and costs, resulting in fewer records compared to transactional or investment data.\n",
    "Recommended Dataset Sizes (for your dummy data project). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from faker import Faker\n",
    "import random\n",
    "\n",
    "fake = Faker()\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(91942)\n",
    "random_state = random.seed(91942)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Dataset 1: ISTO Financial Transactions\n",
    "Purpose: Analyze spending patterns, identify cost-saving opportunities, and visualize financial trends.\n",
    "Table Type: Fact table\n",
    "\n",
    "**Schema:**\n",
    "- Transaction_ID (unique identifier) VARCHAR\n",
    "- Transaction_Date DATETIME\n",
    "- Transaction_Amount FLOAT\n",
    "- Transaction_Type (Expense, Revenue) VARCHAR\n",
    "- Department (e.g., Operations, Investments, Community Programs) VARCHAR\n",
    "- Vendor_Name VARCHAR\n",
    "- Description VARCHAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Transaction_ID Transaction_Date  Transaction_Amount Transaction_Type  \\\n",
      "0    TXN00000001       2023-06-17            57080.47          Expense   \n",
      "1    TXN00000002       2024-03-09           100948.94          Expense   \n",
      "2    TXN00000003       2024-03-13            82489.87          Expense   \n",
      "3    TXN00000004       2023-06-27           149280.56          Revenue   \n",
      "4    TXN00000005       2023-09-23            34606.19          Expense   \n",
      "\n",
      "    Department                 Vendor_Name      Description  \n",
      "0  Investments  Conner, Foster and Johnson      Maintenance  \n",
      "1   Compliance   Walker, Burke and Baldwin  Office Supplies  \n",
      "2   Operations              Poole and Sons  Travel Expenses  \n",
      "3  Investments                   Cox-Jones    Grant Funding  \n",
      "4           IT     Conley, Brown and Perry  Office Supplies  \n",
      "Total transactions: 10000\n",
      "Unique vendors: 954\n",
      "Vendors appearing more than once: 667\n"
     ]
    }
   ],
   "source": [
    "# Number of rows\n",
    "num_rows = 10000\n",
    "num_unique_vendors = 1159 \n",
    "\n",
    "# Generate Transaction IDs (formatted as TXN00000001)\n",
    "transaction_ids = [f'TXN{str(i).zfill(8)}' for i in range(1, num_rows + 1)]\n",
    "\n",
    "# Generate random dates within a realistic range (e.g., past 2 years)\n",
    "transaction_dates = [fake.date_between(start_date='-2y', end_date='today') for _ in range(num_rows)]\n",
    "\n",
    "# Generate transaction amounts (expenses and revenues)\n",
    "transaction_amounts = np.round(np.random.uniform(50, 150000, num_rows), 2)\n",
    "\n",
    "# Randomly assign transaction types\n",
    "transaction_types = np.random.choice(['Expense', 'Revenue'], size=num_rows, p=[0.7, 0.3])\n",
    "\n",
    "# Departments\n",
    "departments = ['Operations', 'Investments', 'Community Programs', 'Administration', 'IT', 'Compliance']\n",
    "department_choices = np.random.choice(departments, size=num_rows)\n",
    "\n",
    "# Vendor names (using Faker), limiting to lower unique vendors\n",
    "unique_vendors = [fake.unique.company() for _ in range(num_unique_vendors)]\n",
    "\n",
    "# Create a realistic distribution of vendor frequencies\n",
    "vendor_probabilities = np.random.zipf(a=2, size=num_unique_vendors)\n",
    "vendor_probabilities = vendor_probabilities / vendor_probabilities.sum()\n",
    "\n",
    "# Assign vendors to transactions based on probabilities\n",
    "vendor_names = np.random.choice(unique_vendors, size=num_rows, p=vendor_probabilities)\n",
    "\n",
    "# Transaction descriptions (simple examples)\n",
    "descriptions_expense = ['Office Supplies', 'Consulting Fees', 'Software Subscription', 'Event Sponsorship', 'Maintenance', 'Travel Expenses']\n",
    "descriptions_revenue = ['Interest Income', 'Investment Returns', 'Program Fees', 'Grant Funding', 'Reimbursement']\n",
    "\n",
    "descriptions = [\n",
    "    random.choice(descriptions_expense) if t == 'Expense' else random.choice(descriptions_revenue)\n",
    "    for t in transaction_types\n",
    "]\n",
    "\n",
    "# Assemble DataFrame\n",
    "transactions_df = pd.DataFrame({\n",
    "    'Transaction_ID': transaction_ids,\n",
    "    'Transaction_Date': transaction_dates,\n",
    "    'Transaction_Amount': transaction_amounts,\n",
    "    'Transaction_Type': transaction_types,\n",
    "    'Department': department_choices,\n",
    "    'Vendor_Name': vendor_names,\n",
    "    'Description': descriptions\n",
    "})\n",
    "\n",
    "# Preview the dataset\n",
    "print(transactions_df.head())\n",
    "print(\"Total transactions:\", len(transactions_df))\n",
    "print(\"Unique vendors:\", transactions_df.Vendor_Name.nunique())\n",
    "print(\"Vendors appearing more than once:\", (transactions_df.Vendor_Name.value_counts() > 1).sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has created a perfect dataframe with no errors or duplicates. This is unrealistic, so let's shake it up.\n",
    "\n",
    "1. Introduce dupes\n",
    "2. Add incorrect data types\n",
    "3. Add logical inconsistencies\n",
    "4. Give it some outliers!\n",
    "5. Mess up the vendor names including INC\n",
    "6. Remove a few IDs\n",
    "7. Spread some NaN love throughout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Transaction_ID            Transaction_Date  Transaction_Amount  \\\n",
      "0    TXN00003763                  2024-01-12            49388.52   \n",
      "1    TXN00007941  2025-09-29 13:32:35.851475            52924.48   \n",
      "2    TXN00007395                  2025-03-02           140902.59   \n",
      "3    TXN00009310                  2024-06-22            60473.14   \n",
      "4    TXN00009084                  2025-04-13           137488.96   \n",
      "\n",
      "  Transaction_Type      Department                 Vendor_Name  \\\n",
      "0          Expense  Administration  Conner, Foster and Johnson   \n",
      "1          Expense      Compliance      Cole, Nunez and Harris   \n",
      "2          Expense  Administration                Tran-Sanchez   \n",
      "3          Expense      Compliance                Garza-Bright   \n",
      "4          Expense  Administration              Serrano-Butler   \n",
      "\n",
      "             Description  \n",
      "0  Software Subscription  \n",
      "1        Consulting Fees  \n",
      "2        Office Supplies  \n",
      "3        Office Supplies  \n",
      "4      Event Sponsorship  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\justi\\AppData\\Local\\Temp\\ipykernel_1724\\2656578920.py:19: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['103538.4' '71506.15' '120225.49' '129209.59' '69280.12' '100484.86'\n",
      " '82174.83' '114616.42' '55552.2' '135823.58' '52923.34' '62042.56'\n",
      " '120506.57' '79590.86' '123237.36' '20528.35' '35164.55' '116512.55'\n",
      " '138830.93' '133164.12' '141469.84' '144046.54' '32740.77' '93226.17'\n",
      " '138094.14' '101212.61' '44310.88' '10794.66' '32747.58' '145253.56'\n",
      " '6716.69' '124463.3' '121659.2' '100058.6' '114127.4' '40838.84'\n",
      " '124203.3' '43014.97' '110460.07' '76922.37' '110888.29' '142715.75'\n",
      " '78947.16' '71146.91' '65448.27' '10842.02' '71859.01' '137943.32'\n",
      " '105404.39' '48441.65' '26752.73' '98718.03' '97671.68' '126678.32'\n",
      " '85428.61' '141451.76' '126947.52' '67966.94' '110043.76' '27480.37'\n",
      " '143568.09' '126267.27' '91074.24' '12732.57' '147709.91' '106504.73'\n",
      " '131184.36' '119501.58' '70779.29' '144606.92' '28477.3' '39193.91'\n",
      " '2936.43' '66219.16' '95026.14' '13196.35' '10666.3' '149190.28'\n",
      " '93285.37' '143967.15' '30004.57' '12659.6' '57597.21' '47684.48'\n",
      " '58539.69' '53962.98' '37755.09' '125014.3' '52827.6' '79804.19'\n",
      " '44292.66' '128304.24' '675.13' '64992.37' '83883.21' '87504.83'\n",
      " '28242.64' '109283.16' '44569.81' '49227.38' '73793.15' '66758.74'\n",
      " '33005.95' '127947.32' '141845.85' '89208.44' '98210.59' '99843.73'\n",
      " '13632.09' '117556.54' '123569.35' '46391.97' '83317.16' '131446.53'\n",
      " '12253.96' '131395.9' '110722.13' '73531.21' '97230.24' '58914.35'\n",
      " '45175.29' '28406.54' '34759.87' '103508.31' '49589.3' '147916.28'\n",
      " '85549.95' '114417.01' '92965.54' '19520.96' '78716.58' '86414.23'\n",
      " '63925.59' '83830.83' '139771.51' '83462.14' '71602.97' '119145.13'\n",
      " '141723.16' '7752.64' '136609.91' '115930.17' '80920.75' '121181.74'\n",
      " '43711.08' '121479.78' '76196.8' '99514.95' '51250.45' '62568.36'\n",
      " '86349.94' '64997.55' '123567.97' '93866.73' '135701.85' '47430.31'\n",
      " '30103.51' '124179.53' '141255.99' '69994.11' '65162.24' '88137.72'\n",
      " '78123.5' '110264.52' '76247.35' '91298.41' '147662.05' '95351.67'\n",
      " '135330.94' '330.08' '64355.59' '96103.89' '66461.26' '106527.57'\n",
      " '50154.04' '93809.08' '103029.18' '10529.39' '2355.35' '44544.96'\n",
      " '64449.98' '7177.01' '28788.4' '22510.56' '65043.92' '82617.41'\n",
      " '140712.12' '110609.52' '138225.98' '129964.04' '31550.47' '3392.96'\n",
      " '42123.0' '130798.29' '56171.7' '82611.86' '123601.96' '106855.6'\n",
      " '108231.23' '98660.26' '111559.38' '147416.97' '113586.49' '114902.75']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  transactions_df.loc[type_error_indices, 'Transaction_Amount'] = transactions_df.loc[type_error_indices, 'Transaction_Amount'].astype(str)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "np.random.seed(91942)\n",
    "random_state = random.seed(91942)\n",
    "random_state_alt = random.seed(24919)\n",
    "\n",
    "transactions_clean_df = copy.deepcopy(transactions_df)\n",
    "\n",
    "# 1. Introduce duplicate rows\n",
    "num_duplicates = int(len(transactions_df) * 0.02)  # 2% duplicates\n",
    "duplicates_df = transactions_df.sample(n=num_duplicates, random_state=random_state)\n",
    "transactions_df = pd.concat([transactions_df, duplicates_df], ignore_index=True)\n",
    "\n",
    "# 2. Introduce incorrect data types\n",
    "num_type_errors = int(len(transactions_df) * 0.02)  # 2% type errors\n",
    "type_error_indices = transactions_df.sample(n=num_type_errors, random_state=random_state).index\n",
    "transactions_df.loc[type_error_indices, 'Transaction_Amount'] = transactions_df.loc[type_error_indices, 'Transaction_Amount'].astype(str)\n",
    "\n",
    "date_error_indices = transactions_df.sample(n=num_type_errors, random_state=random_state_alt).index\n",
    "transactions_df.loc[date_error_indices, 'Transaction_Date'] = transactions_df.loc[date_error_indices, 'Transaction_Date'].astype(str) + ' INVALID'\n",
    "\n",
    "# 3. Introduce logical inconsistencies\n",
    "expense_indices = transactions_df[transactions_df['Transaction_Type'] == 'Expense'].sample(n=num_type_errors, random_state=random_state).index\n",
    "transactions_df.loc[expense_indices, 'Transaction_Amount'] *= -1  # Negative amounts for expenses\n",
    "\n",
    "invalid_department_indices = transactions_df.sample(n=num_type_errors, random_state=random_state_alt).index\n",
    "transactions_df.loc[invalid_department_indices, 'Department'] = 'InvalidDept'\n",
    "\n",
    "# 4. Introduce outliers\n",
    "# Ensure 'Transaction_Amount' is numeric\n",
    "transactions_df['Transaction_Amount'] = pd.to_numeric(transactions_df['Transaction_Amount'], errors='coerce')\n",
    "outlier_indices = transactions_df.sample(n=num_type_errors, random_state=random_state).index\n",
    "\n",
    "# Generate outliers using a log-normal distribution\n",
    "outlier_values = np.random.lognormal(mean=10, sigma=2, size=num_type_errors)\n",
    "transactions_df.loc[outlier_indices, 'Transaction_Amount'] = outlier_values\n",
    "\n",
    "future_date_indices = transactions_df.sample(n=num_type_errors, random_state=random_state_alt).index\n",
    "transactions_df.loc[future_date_indices, 'Transaction_Date'] = pd.Timestamp('today') + pd.to_timedelta(np.random.randint(30, 365, size=num_type_errors), unit='d')\n",
    "\n",
    "# 5. Introduce inconsistent vendor names\n",
    "vendor_indices = transactions_df.sample(n=num_type_errors, random_state=random_state).index\n",
    "transactions_df.loc[vendor_indices, 'Vendor_Name'] = transactions_df.loc[vendor_indices, 'Vendor_Name'].apply(\n",
    "    lambda x: x.replace('Inc.', 'Incorporated') if 'Inc.' in x else x + ' Inc.'\n",
    ")\n",
    "\n",
    "# 6. Introduce missing or invalid Transaction IDs\n",
    "missing_id_indices = transactions_df.sample(n=num_type_errors, random_state=random_state).index\n",
    "transactions_df.loc[missing_id_indices, 'Transaction_ID'] = np.nan\n",
    "\n",
    "# 7. Introduce missing values (nulls) - needs to be last so it doesn't break the multiplication code\n",
    "num_nulls = int(len(transactions_df) * 0.03)  # 3% nulls\n",
    "null_indices_amount = transactions_df.sample(n=num_nulls, random_state=random_state).index\n",
    "null_indices_department = transactions_df.sample(n=num_nulls, random_state=random_state_alt).index\n",
    "transactions_df.loc[null_indices_amount, 'Transaction_Amount'] = np.nan\n",
    "transactions_df.loc[null_indices_department, 'Department'] = np.nan\n",
    "\n",
    "# Shuffle the DataFrame to mix errors naturally\n",
    "transactions_df = transactions_df.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "\n",
    "# Check the resulting DataFrame\n",
    "print(transactions_df.head())\n",
    "# print(transactions_df.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique vendors: 1041\n",
      "Number of duplicated vendors: 9159\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of unique vendors:\", len(transactions_df.Vendor_Name.unique()))\n",
    "print(\"Number of duplicated vendors:\", transactions_df.Vendor_Name.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Transaction_ID', 'Transaction_Date', 'Transaction_Amount',\n",
      "       'Transaction_Type', 'Department', 'Vendor_Name', 'Description'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(transactions_df.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, save the data in a horrible data format: csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV and cringe\n",
    "transactions_df.to_csv('transactions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Dataset #2 Investment Portfolio Holdings\n",
    "Purpose: Analyze ISTO's investment portfolio, diversification, and risk management.\n",
    "\n",
    "Table Type: Dimension\n",
    "\n",
    "**Schema:**\n",
    "- Holding_ID VARCHAR\n",
    "- Security_Name VARCHAR\n",
    "- Security_Type VARCHAR\n",
    "- Quantity_Held INT\n",
    "- Market_Value FLOAT\n",
    "- Acquisition_Date DATETIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Holding_ID    Security_Name Security_Type  Quantity_Held  \\\n",
      "0   HLD00001    Marsh-Stanton         Stock            544   \n",
      "1   HLD00002    Hayes-Griffin          Bond           1106   \n",
      "2   HLD00003  Rodriguez-Davis          Bond           1841   \n",
      "3   HLD00004      Jackson Inc   Mutual Fund           1427   \n",
      "4   HLD00005    Graham-Newman         Stock            805   \n",
      "\n",
      "   Market_Price_Per_Unit  Total_Market_Value Acquisition_Date  \n",
      "0                 493.30           268355.20       2022-05-02  \n",
      "1                  44.65            49382.90       2023-09-23  \n",
      "2                  53.83            99101.03       2023-11-01  \n",
      "3                 424.49           605747.23       2021-11-26  \n",
      "4                  96.97            78060.85       2023-08-07  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from faker import Faker\n",
    "\n",
    "fake = Faker()\n",
    "np.random.seed(91942)\n",
    "\n",
    "# Number of holdings\n",
    "num_holdings = 257\n",
    "\n",
    "# Generate Holding IDs\n",
    "holding_ids = [f'HLD{str(i).zfill(5)}' for i in range(1, num_holdings + 1)]\n",
    "\n",
    "# Generate realistic security names (e.g., company names)\n",
    "security_names = [fake.unique.company() for _ in range(num_holdings)]\n",
    "\n",
    "# Security types distribution\n",
    "security_types = np.random.choice(\n",
    "    ['Stock', 'Bond', 'ETF', 'Mutual Fund'],\n",
    "    size=num_holdings,\n",
    "    p=[0.5, 0.2, 0.2, 0.1]\n",
    ")\n",
    "\n",
    "# Quantity held (realistic quantities)\n",
    "quantity_held = np.random.randint(100, 5000, size=num_holdings)\n",
    "\n",
    "# Market value per unit (realistic prices)\n",
    "market_price_per_unit = np.round(np.random.uniform(10, 500, size=num_holdings), 2)\n",
    "\n",
    "# Calculate total market value\n",
    "market_values = np.round(quantity_held * market_price_per_unit, 2)\n",
    "\n",
    "# Acquisition dates (within past 5 years)\n",
    "acquisition_dates = pd.to_datetime([fake.date_between(start_date='-5y', end_date='today') for _ in range(num_holdings)])\n",
    "\n",
    "\n",
    "# Assemble DataFrame\n",
    "portfolio_holdings_df = pd.DataFrame({\n",
    "    'Holding_ID': holding_ids,\n",
    "    'Security_Name': security_names,\n",
    "    'Security_Type': security_types,\n",
    "    'Quantity_Held': quantity_held,\n",
    "    'Market_Price_Per_Unit': market_price_per_unit,\n",
    "    'Total_Market_Value': market_values,\n",
    "    'Acquisition_Date': acquisition_dates\n",
    "})\n",
    "\n",
    "# Preview the dataset\n",
    "print(portfolio_holdings_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Transaction_ID', 'Transaction_Date', 'Transaction_Amount',\n",
      "       'Transaction_Type', 'Department', 'Vendor_Name', 'Description'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(transactions_df.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a perfect dataset, let's make it crappy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Holding_ID    Security_Name Security_Type  Quantity_Held  \\\n",
      "0   HLD00144  Fisher-Martinez         Stock           2908   \n",
      "1   HLD00145        Moore Ltd           ETF           2357   \n",
      "2   HLD00226    Vargas-Harris         Stock           2428   \n",
      "3   HLD00182      Michael LLC   Mutual Fund           4352   \n",
      "4   HLD00239      Young Group          Bond           4716   \n",
      "\n",
      "   Market_Price_Per_Unit  Total_Market_Value Acquisition_Date  \n",
      "0                 157.64           458417.12       2021-02-06  \n",
      "1                 487.14          1148188.98       2020-11-22  \n",
      "2                 201.93           490286.04       2024-03-18  \n",
      "3                 492.02          2141271.04       2021-02-11  \n",
      "4                 426.09          2009440.44       2023-12-13  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from faker import Faker\n",
    "\n",
    "fake = Faker()\n",
    "np.random.seed(91942)\n",
    "random_state=random.seed(91942)\n",
    "random_state_alt = random.seed(24919)\n",
    "\n",
    "# 1. Introduce duplicate holdings with slight variations (3%)\n",
    "num_duplicates = int(len(portfolio_holdings_df) * 0.03)\n",
    "duplicates_df = portfolio_holdings_df.sample(n=num_duplicates, random_state=random_state).copy()\n",
    "\n",
    "duplicates_df['Security_Name'] = duplicates_df['Security_Name'].apply(\n",
    "    lambda x: x + ' Inc.' if 'Inc.' not in x else x.replace('Inc.', 'Incorporated')\n",
    ")\n",
    "duplicates_df['Acquisition_Date'] += pd.to_timedelta(\n",
    "    np.random.randint(-10, 10, size=num_duplicates), unit='d'\n",
    ")\n",
    "\n",
    "# 2. Introduce missing values in 'Market_Price_Per_Unit' (2%)\n",
    "num_nulls = int(len(portfolio_holdings_df) * 0.02)\n",
    "null_indices = portfolio_holdings_df.sample(n=num_nulls, random_state=random_state).index\n",
    "portfolio_holdings_df.loc[null_indices, 'Market_Price_Per_Unit'] = np.nan\n",
    "\n",
    "# 3. Introduce unrealistic market prices (outliers) (2%)\n",
    "num_outliers = int(len(portfolio_holdings_df) * 0.02)\n",
    "outlier_indices = portfolio_holdings_df.sample(n=num_outliers, random_state=random_state_alt).index\n",
    "portfolio_holdings_df.loc[outlier_indices, 'Market_Price_Per_Unit'] *= np.random.choice([0.01, 100], size=num_outliers)\n",
    "portfolio_holdings_df['Total_Market_Value'] = portfolio_holdings_df['Quantity_Held'] * portfolio_holdings_df['Market_Price_Per_Unit']\n",
    "\n",
    "# 4. Introduce incorrect security types (misclassification) (3%)\n",
    "num_misclassified = int(len(portfolio_holdings_df) * 0.03)\n",
    "misclassified_indices = portfolio_holdings_df.sample(n=num_misclassified, random_state=random_state).index\n",
    "portfolio_holdings_df.loc[misclassified_indices, 'Security_Type'] = np.random.choice(['Stock', 'Bond', 'ETF', 'Mutual Fund'], size=num_misclassified)\n",
    "\n",
    "# 5. Introduce future acquisition dates (2%)\n",
    "num_future_dates = int(len(portfolio_holdings_df) * 0.02)\n",
    "future_indices = portfolio_holdings_df.sample(n=num_future_dates, random_state=random_state_alt).index\n",
    "portfolio_holdings_df.loc[future_indices, 'Acquisition_Date'] = pd.Timestamp('today') + pd.to_timedelta(np.random.randint(30, 365, size=num_future_dates), unit='d')\n",
    "\n",
    "# Combine duplicates back into the original DataFrame\n",
    "portfolio_holdings_df = pd.concat([portfolio_holdings_df, duplicates_df], ignore_index=True)\n",
    "\n",
    "# Shuffle the DataFrame to mix errors naturally\n",
    "portfolio_holdings_df = portfolio_holdings_df.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "\n",
    "# Check the resulting DataFrame\n",
    "print(portfolio_holdings_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV and cringe some more\n",
    "portfolio_holdings_df.to_csv('portfolio_holdings.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Dataset #3: External Vendor Information\n",
    "Contains descriptive attributes about the vendors, and provides context for analyzing transactions from the transaction table. \n",
    "\n",
    "Table Type: Dimension\n",
    "\n",
    "**Schema:**\n",
    "- Vendor_ID VARHCAR\n",
    "- Vendor_Name VARCHAR\n",
    "- Service Type VARCHAR\n",
    "- Contract_Start_Date DATETIME\n",
    "- Contract_End_Date DATETIME\n",
    "- Amount_Paid_YTD FLOAT\n",
    "\n",
    "\n",
    "Considerations: \n",
    "Generating this table has a few dependencies: \n",
    "1. The external vendors with contracts should be a subset of the vendors making transactions\n",
    "2. If it is an external vendor with a contract, they should not be making transactions AFTER their contract has ended\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Vendor_ID                 Vendor_Name Service_Type Contract_Start_Date  \\\n",
      "0   VND0001             Taylor-Robinson   Consulting          2020-10-28   \n",
      "1   VND0002            Mitchell-Sanchez     Supplies          2022-12-05   \n",
      "2   VND0003               Hardy-Swanson  Maintenance          2021-09-29   \n",
      "3   VND0004             Benjamin-Medina  Maintenance          2023-07-15   \n",
      "4   VND0005  Herrera, Campbell and Rios     Supplies          2022-08-21   \n",
      "\n",
      "           Contract_End_Date  Amount_Paid_YTD  \n",
      "0 2025-04-18 13:32:36.319020         89484.49  \n",
      "1 2024-08-09 00:00:00.000000         75736.47  \n",
      "2 2025-04-18 13:32:36.319020         44447.81  \n",
      "3 2024-11-10 00:00:00.000000         16022.84  \n",
      "4 2025-04-18 13:32:36.319020         88657.40  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from faker import Faker\n",
    "\n",
    "# Initialize Faker for generating random data\n",
    "fake = Faker()\n",
    "np.random.seed(91942)\n",
    "\n",
    "# Number of vendors to include in the External Vendor Information Table\n",
    "num_vendors = 200\n",
    "\n",
    "# Uses the unique_vendors from the Transactions Table\n",
    "external_vendor_subset = np.random.choice(unique_vendors, size=num_vendors, replace=False)\n",
    "\n",
    "# Generate unique Vendor IDs for the subset\n",
    "vendor_ids = [f'VND{str(i).zfill(4)}' for i in range(1, num_vendors + 1)]\n",
    "\n",
    "# Generate random Service Types\n",
    "service_types = np.random.choice(\n",
    "    ['IT Services', 'Consulting', 'Supplies', 'Financial Services', 'Maintenance'],\n",
    "    size=num_vendors\n",
    ")\n",
    "\n",
    "# Generate random Contract Start Dates (within the past 5 years)\n",
    "contract_start_dates = pd.to_datetime(\n",
    "    [fake.date_between(start_date='-5y', end_date='today') for _ in range(num_vendors)]\n",
    ")\n",
    "\n",
    "# Generate random Contract End Dates (1â€“3 years after the start date)\n",
    "contract_end_dates = contract_start_dates + pd.to_timedelta(\n",
    "    np.random.randint(365, 1095, size=num_vendors), unit='d'\n",
    ")\n",
    "\n",
    "# Simulate Transactions DataFrame to get the latest transaction date for each vendor\n",
    "# Generate transaction dates within the past 2 years\n",
    "transaction_dates = [fake.date_between(start_date='-2y', end_date='today') for _ in range(10000)]\n",
    "\n",
    "# Generate a single Zipf distribution for vendor probabilities\n",
    "zipf_distribution = np.random.zipf(a=2, size=1159)\n",
    "vendor_probabilities = zipf_distribution / zipf_distribution.sum()\n",
    "\n",
    "# Assign vendors to transactions based on the normalized probabilities\n",
    "transaction_vendors = np.random.choice(unique_vendors, size=10000, p=vendor_probabilities)\n",
    "\n",
    "# Create the Transactions DataFrame\n",
    "transactions_vendors_df = pd.DataFrame({\n",
    "    'Transaction_Date': transaction_dates,\n",
    "    'Vendor_Name': transaction_vendors\n",
    "})\n",
    "\n",
    "# Ensure Contract End Dates are after the latest transaction date for each vendor\n",
    "latest_transaction_dates = transactions_vendors_df.groupby('Vendor_Name')['Transaction_Date'].max().apply(pd.Timestamp)\n",
    "contract_end_dates = [\n",
    "    max(latest_transaction_dates.get(vendor, pd.Timestamp('today')), end_date)\n",
    "    for vendor, end_date in zip(external_vendor_subset, contract_end_dates)\n",
    "]\n",
    "\n",
    "# Generate random Amount Paid Year-to-Date (YTD)\n",
    "amount_paid_ytd = np.round(np.random.uniform(5000, 200000, size=num_vendors), 2)\n",
    "\n",
    "# Create the External Vendor Information DataFrame\n",
    "external_vendor_df = pd.DataFrame({\n",
    "    'Vendor_ID': vendor_ids,\n",
    "    'Vendor_Name': external_vendor_subset,\n",
    "    'Service_Type': service_types,\n",
    "    'Contract_Start_Date': contract_start_dates,\n",
    "    'Contract_End_Date': contract_end_dates,\n",
    "    'Amount_Paid_YTD': amount_paid_ytd\n",
    "})\n",
    "\n",
    "# Preview the table\n",
    "print(external_vendor_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we have perfect data that we need to ruin by \n",
    "Adding duplicate rows\n",
    "Inconsistent date formats\n",
    "Outliers\n",
    "Logical inconsistencies\n",
    "Inconsistent naming conventions\n",
    "missing IDs\n",
    "Null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Vendor_ID                  Vendor_Name        Service_Type  \\\n",
      "0    VND0001              Taylor-Robinson          Consulting   \n",
      "1    VND0002             Mitchell-Sanchez            Supplies   \n",
      "2    VND0003                Hardy-Swanson         Maintenance   \n",
      "3    VND0004              Benjamin-Medina         Maintenance   \n",
      "4    VND0005   Herrera, Campbell and Rios            Supplies   \n",
      "5    VND0006                Flores-Weaver         Maintenance   \n",
      "6    VND0007                Stewart-Green         Maintenance   \n",
      "7    VND0008     Pace, Henry and Williams  Financial Services   \n",
      "8    VND0009                 Roman-Taylor         Maintenance   \n",
      "9    VND0010                   RHODES INC          Consulting   \n",
      "10   VND0011             Cunningham Group         IT Services   \n",
      "11   VND0012      Lara, Morgan and Warren            Supplies   \n",
      "12   VND0013  Wilson, Stewart and Padilla  Financial Services   \n",
      "13   VND0014                Park and Sons         Maintenance   \n",
      "14      None    Brown, Johnson and Bowers                None   \n",
      "15   VND0016    miller, rivera and romero         Maintenance   \n",
      "16   VND0017             Gonzales-Johnson  Financial Services   \n",
      "17   VND0018                    Poole LLC            Supplies   \n",
      "18   VND0019   Perry, Castro and Gonzalez         IT Services   \n",
      "19   VND0020               Richard-Powers  Financial Services   \n",
      "\n",
      "   Contract_Start_Date          Contract_End_Date  Amount_Paid_YTD  \n",
      "0           2020-10-28 2025-04-18 13:32:36.319020     8.948449e+04  \n",
      "1           2022-12-05 2024-08-09 00:00:00.000000     7.573647e+04  \n",
      "2           2021-09-29 2025-04-18 13:32:36.319020     4.444781e+04  \n",
      "3           2023-07-15 2024-11-10 00:00:00.000000     1.602284e+04  \n",
      "4           2022-08-21 2025-04-18 13:32:36.319020     8.865740e+04  \n",
      "5           2024-12-02 2026-03-14 00:00:00.000000     3.182959e+04  \n",
      "6           2022-09-09 2024-11-29 00:00:00.000000     1.592402e+05  \n",
      "7           2024-08-10 2027-06-07 00:00:00.000000     1.348047e+05  \n",
      "8           2024-12-02 2026-02-20 00:00:00.000000     1.939453e+05  \n",
      "9           2023-02-15                        NaT     2.508643e+02  \n",
      "10          2024-02-02 2025-03-09 00:00:00.000000     1.939018e+05  \n",
      "11          2024-02-22 2026-12-13 00:00:00.000000     8.527902e+04  \n",
      "12          2020-12-19 2023-11-13 00:00:00.000000     1.787812e+05  \n",
      "13          2020-07-27 2025-04-18 13:32:36.319020     1.503632e+05  \n",
      "14          2023-04-04 2025-04-18 13:32:36.319020     1.838942e+05  \n",
      "15          2022-09-15                        NaT     1.108611e+07  \n",
      "16          2021-03-20 2025-04-18 13:32:36.319020     1.001502e+05  \n",
      "17          2022-09-19 2025-04-18 13:32:36.319020     5.012740e+03  \n",
      "18          2020-12-09 2025-01-23 00:00:00.000000     1.858419e+05  \n",
      "19          2021-12-09 2024-05-30 00:00:00.000000     1.640661e+05  \n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Introduce Duplicate Rows\n",
    "num_duplicates = int(len(external_vendor_df) * 0.02)  # 2% duplicates\n",
    "duplicates_df = external_vendor_df.sample(n=num_duplicates, random_state=42)\n",
    "external_vendor_df = pd.concat([external_vendor_df, duplicates_df], ignore_index=True)\n",
    "\n",
    "# Introduce Inconsistent Data Formats\n",
    "# Convert some dates to strings\n",
    "num_inconsistent_dates = int(len(external_vendor_df) * 0.05)  # 5% inconsistent dates\n",
    "date_format_indices = external_vendor_df.sample(n=num_inconsistent_dates, random_state=42).index\n",
    "external_vendor_df.loc[date_format_indices, 'Contract_Start_Date'] = external_vendor_df.loc[\n",
    "    date_format_indices, 'Contract_Start_Date'\n",
    "].astype(str)\n",
    "\n",
    "# Modify Vendor Names to Introduce Inconsistencies\n",
    "vendor_name_indices = external_vendor_df.sample(n=num_inconsistent_dates, random_state=42).index\n",
    "external_vendor_df.loc[vendor_name_indices, 'Vendor_Name'] = external_vendor_df.loc[\n",
    "    vendor_name_indices, 'Vendor_Name'\n",
    "].apply(lambda x: x.lower() if random.random() > 0.5 else x.upper())\n",
    "\n",
    "# Introduce Outliers in Amount_Paid_YTD\n",
    "outlier_indices = external_vendor_df.sample(n=num_inconsistent_dates, random_state=42).index\n",
    "external_vendor_df.loc[outlier_indices, 'Amount_Paid_YTD'] = external_vendor_df.loc[\n",
    "    outlier_indices, 'Amount_Paid_YTD'\n",
    "] * np.random.choice([0.01, 100], size=num_inconsistent_dates)\n",
    "\n",
    "# Introduce Logical Inconsistencies\n",
    "# Set Contract_End_Date earlier than Contract_Start_Date\n",
    "logical_error_indices = external_vendor_df.sample(n=num_inconsistent_dates, random_state=42).index\n",
    "external_vendor_df.loc[logical_error_indices, 'Contract_End_Date'] = external_vendor_df.loc[\n",
    "    logical_error_indices, 'Contract_Start_Date'\n",
    "] - pd.Timedelta(days=random.randint(1, 365))\n",
    "\n",
    "# Introduce Missing Values (Nulls) - LAST STEP\n",
    "num_nulls = int(len(external_vendor_df) * 0.05)  # 5% nulls\n",
    "null_indices_end_date = external_vendor_df.sample(n=num_nulls, random_state=42).index\n",
    "null_indices_service_type = external_vendor_df.sample(n=num_nulls, random_state=24).index\n",
    "null_indices_vendor_id = external_vendor_df.sample(n=num_nulls, random_state=36).index\n",
    "\n",
    "external_vendor_df.loc[null_indices_end_date, 'Contract_End_Date'] = pd.NaT\n",
    "external_vendor_df.loc[null_indices_service_type, 'Service_Type'] = None\n",
    "external_vendor_df.loc[null_indices_vendor_id, 'Vendor_ID'] = None\n",
    "\n",
    "# Preview the table with errors\n",
    "print(external_vendor_df.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Save the table to a CSV file\n",
    "external_vendor_df.to_csv('external_vendor_information.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Dataset #4: Unclaimed Property Records\n",
    "Purpose: Evaluate effectiveness of ISTO's unclaimed property program, identify trends, and visualize property types.\n",
    "\n",
    "Table Type: Fact Table\n",
    "\n",
    "**Schema:** \n",
    "- Property_ID VARCHAR\n",
    "- Owner_Name VARCHAR\n",
    "- Property_Type VARCHAR\n",
    "- Reported_Date DATETIME\n",
    "- Property_Value FLOAT\n",
    "- Claim_Status VARCHAR\n",
    "\n",
    "Considerations: \n",
    "1. There will be a mix of vendors and people who own unclaimed property\n",
    "2. Some of the vendors will be active vendors, but some will not be active, and will not appear in any of the tables (perhaps former contract holders)\n",
    "3. Dates will only be investigated quarterly; this isn't a daily investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Property_ID  Owner_Type                     Owner_Name     Property_Type  \\\n",
      "0      UP0001      Vendor  Carroll, Rodriguez and Morgan  Safe Deposit Box   \n",
      "1      UP0002  Individual                     Kyle Stark      Bank Account   \n",
      "2      UP0003  Individual             Kelly Robinson DDS   Insurance Claim   \n",
      "3      UP0004  Individual                   Dylan Bryant  Safe Deposit Box   \n",
      "4      UP0005  Individual             Christopher Chavez   Insurance Claim   \n",
      "\n",
      "  Reported_Date  Property_Value Claim_Status  \n",
      "0    2018-09-30         5011.73    Unclaimed  \n",
      "1    2022-03-31         8860.79    Unclaimed  \n",
      "2    2022-03-31         6253.99    Unclaimed  \n",
      "3    2018-12-31         7916.36    Unclaimed  \n",
      "4    2018-12-31          443.81    Unclaimed  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\justi\\AppData\\Local\\Temp\\ipykernel_1724\\3275233515.py:36: FutureWarning: 'Q' is deprecated and will be removed in a future version, please use 'QE' instead.\n",
      "  quarters = pd.date_range(start='2018-01-01', end='2023-12-31', freq='Q')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from faker import Faker\n",
    "\n",
    "# Initialize Faker for generating random data\n",
    "fake = Faker()\n",
    "np.random.seed(91942)\n",
    "\n",
    "# Number of unclaimed property records\n",
    "num_records = 200\n",
    "\n",
    "# Use active vendors from the Transactions Table\n",
    "# unique_vendors = unique_vendors # from the Transactions processing\n",
    "active_vendors = np.random.choice(unique_vendors, size=int(num_records * 0.1), replace=False)  # 40% active vendors\n",
    "\n",
    "# Generate new vendors (inactive vendors)\n",
    "inactive_vendors = [fake.company() for _ in range(int(num_records * 0.4))]  # 30% inactive vendors\n",
    "\n",
    "# Generate individuals \n",
    "individuals = [fake.name() for _ in range(int(num_records * 0.5))]\n",
    "\n",
    "# Combine active and inactive vendors\n",
    "owner_names = np.concatenate([active_vendors, inactive_vendors, individuals])\n",
    "np.random.shuffle(owner_names)  # Shuffle to mix active and inactive vendors\n",
    "\n",
    "# Generate random Owner Types (Individual or Vendor)\n",
    "owner_types = ['Vendor' if owner in active_vendors or owner in inactive_vendors else \"Individual\" for owner in owner_names]\n",
    "\n",
    "# Generate random Property Types\n",
    "property_types = np.random.choice(\n",
    "    ['Bank Account', 'Insurance Claim', 'Stocks', 'Safe Deposit Box', 'Uncashed Check', 'Bonds'],\n",
    "    size=num_records\n",
    ")\n",
    "\n",
    "# Generate random Reported Dates (limited to quarterly intervals in the past 5 years)\n",
    "quarters = pd.date_range(start='2018-01-01', end='2023-12-31', freq='Q')\n",
    "reported_dates = np.random.choice(quarters, size=num_records)\n",
    "\n",
    "# Generate random Property Values\n",
    "property_values = np.round(np.random.uniform(100, 10000, size=num_records), 2)\n",
    "\n",
    "# Generate random Claim Status\n",
    "claim_status = np.random.choice(['Claimed', 'Unclaimed'], size=num_records, p=[0.3, 0.7])\n",
    "\n",
    "# Create the Unclaimed Property Records DataFrame\n",
    "unclaimed_property_df = pd.DataFrame({\n",
    "    'Property_ID': [f'UP{str(i).zfill(4)}' for i in range(1, num_records + 1)],\n",
    "    'Owner_Type': owner_types,\n",
    "    'Owner_Name': owner_names,\n",
    "    'Property_Type': property_types,\n",
    "    'Reported_Date': reported_dates,\n",
    "    'Property_Value': property_values,\n",
    "    'Claim_Status': claim_status\n",
    "})\n",
    "\n",
    "# Preview the table\n",
    "print(unclaimed_property_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oooooh, nice data again. Let's rain on this parade with some duplicates, inconsistencies, outliers and nulls! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Property_ID  Owner_Type                     Owner_Name     Property_Type  \\\n",
      "0       UP0001      Vendor  Carroll, Rodriguez and Morgan  Safe Deposit Box   \n",
      "1       UP0002  Individual                     Kyle Stark      Bank Account   \n",
      "2       UP0003  Individual             Kelly Robinson DDS   Insurance Claim   \n",
      "3       UP0004  Individual                   Dylan Bryant  Safe Deposit Box   \n",
      "4       UP0005  Individual             Christopher Chavez   Insurance Claim   \n",
      "5       UP0006      Vendor                Brennan-Leonard   Insurance Claim   \n",
      "6       UP0007      Vendor     Perry, Castro and Gonzalez    Uncashed Check   \n",
      "7       UP0008  Individual                 Albert Schultz    Uncashed Check   \n",
      "8       UP0009      Vendor       Smith, Lopez and Sanders             Bonds   \n",
      "9       UP0010      Vendor         Knight, Reed and Price    Uncashed Check   \n",
      "10      UP0011  Individual                   Craig Murphy    Uncashed Check   \n",
      "11      UP0012      Vendor    Foster, Romero and Harrison            Stocks   \n",
      "12      UP0013  Individual                   Timothy Rich    Uncashed Check   \n",
      "13      UP0014      Vendor          Lin, Graves and Jones             Bonds   \n",
      "14      UP0015      Vendor               Gonzalez-Beasley  Safe Deposit Box   \n",
      "15      UP0016      Vendor                    George-Tran             Bonds   \n",
      "16      UP0017  Individual                  Marissa Moore   Insurance Claim   \n",
      "17      UP0018      Vendor                       Dean Inc   Insurance Claim   \n",
      "18      UP0019      Vendor                           None             Bonds   \n",
      "19      UP0020  Individual                   Sandra Brown            Stocks   \n",
      "\n",
      "   Reported_Date  Property_Value Claim_Status  \n",
      "0     2018-09-30       5011.7300    Unclaimed  \n",
      "1     2022-03-31       8860.7900    Unclaimed  \n",
      "2     2022-03-31       6253.9900    Unclaimed  \n",
      "3     2018-12-31       7916.3600    Unclaimed  \n",
      "4     2018-12-31        443.8100    Unclaimed  \n",
      "5     2022-09-30        195.3900      Claimed  \n",
      "6     2022-03-31       1105.7600    Unclaimed  \n",
      "7     2023-03-31       5789.1600    Unclaimed  \n",
      "8     2023-03-31       6484.3900      Claimed  \n",
      "9     2022-09-30       6958.5200    Unclaimed  \n",
      "10    2020-06-30       3519.3100    Unclaimed  \n",
      "11    2023-06-30       7142.3300    Unclaimed  \n",
      "12    2022-12-31       9611.3100    Unclaimed  \n",
      "13    2023-12-31        588.1200    Unclaimed  \n",
      "14    2022-03-31       4853.3400    Unclaimed  \n",
      "15    2022-03-31       3563.7000      Claimed  \n",
      "16    2022-09-30       3734.6900    Unclaimed  \n",
      "17    2019-03-31         25.8937    Unclaimed  \n",
      "18    2023-06-30       6835.2300    Unclaimed  \n",
      "19    2023-06-30       3138.8700      Claimed  \n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Introduce Duplicate Records\n",
    "num_duplicates = int(len(unclaimed_property_df) * 0.02)  # 2% duplicates\n",
    "duplicates_df = unclaimed_property_df.sample(n=num_duplicates, random_state=random_state)\n",
    "unclaimed_property_df = pd.concat([unclaimed_property_df, duplicates_df], ignore_index=True)\n",
    "\n",
    "# Introduce Inconsistent Formats\n",
    "# Convert some dates to strings\n",
    "num_inconsistent_dates = int(len(unclaimed_property_df) * 0.01)  # 1% inconsistent dates\n",
    "date_format_indices = unclaimed_property_df.sample(n=num_inconsistent_dates, random_state=random_state).index\n",
    "unclaimed_property_df.loc[date_format_indices, 'Reported_Date'] = unclaimed_property_df.loc[\n",
    "    date_format_indices, 'Reported_Date'\n",
    "].astype(str)\n",
    "\n",
    "# Modify Owner Names to Introduce Inconsistencies\n",
    "owner_name_indices = unclaimed_property_df.sample(n=num_inconsistent_dates, random_state=random_state).index\n",
    "unclaimed_property_df.loc[owner_name_indices, 'Owner_Name'] = unclaimed_property_df.loc[\n",
    "    owner_name_indices, 'Owner_Name'\n",
    "].apply(lambda x: x.lower() if isinstance(x, str) and random.random() > 0.5 else x.upper() if isinstance(x, str) else x)\n",
    "\n",
    "# Introduce Logical Inconsistencies\n",
    "# Set Claim_Status to \"Claimed\" but leave Owner_Name as null\n",
    "logical_error_indices = unclaimed_property_df.sample(n=num_inconsistent_dates, random_state=random_state).index\n",
    "unclaimed_property_df.loc[logical_error_indices, 'Claim_Status'] = 'Claimed'\n",
    "unclaimed_property_df.loc[logical_error_indices, 'Owner_Name'] = None\n",
    "\n",
    "# Introduce Outliers in Property_Value\n",
    "outlier_indices = unclaimed_property_df.sample(n=num_inconsistent_dates, random_state=random_state).index\n",
    "unclaimed_property_df.loc[outlier_indices, 'Property_Value'] = unclaimed_property_df.loc[\n",
    "    outlier_indices, 'Property_Value'\n",
    "] * np.random.choice([0.01, 1000], size=num_inconsistent_dates)\n",
    "\n",
    "# Introduce Missing Values (Nulls) - LAST STEP\n",
    "num_nulls = int(len(unclaimed_property_df) * 0.01)  # 1% nulls\n",
    "null_indices_owner_name = unclaimed_property_df.sample(n=num_nulls, random_state=random_state).index\n",
    "null_indices_property_type = unclaimed_property_df.sample(n=num_nulls, random_state=random_state_alt).index\n",
    "\n",
    "unclaimed_property_df.loc[null_indices_owner_name, 'Owner_Name'] = None\n",
    "unclaimed_property_df.loc[null_indices_property_type, 'Property_Type'] = None\n",
    "\n",
    "# Preview the table with errors\n",
    "print(unclaimed_property_df.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Optional: Save the table to a CSV file\n",
    "unclaimed_property_df.to_csv('unclaimed_property_records.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Dataset 5: Program Performance Metrics\n",
    "Purpose: Evaluate effectiveness and impact of ISTO programs, identify areas for improvement.\n",
    "\n",
    "Table Type: Dimension\n",
    "\n",
    "**Schema:**\n",
    "- Program_ID VARCHAR\n",
    "- Program_Name VARCHAR\n",
    "- Reporting_Period DATETIME\n",
    "- Participants VARCHAR\n",
    "- Successful_Outcomes VARCHAR\n",
    "- Program_Cost FLOAT\n",
    "\n",
    "Considerations: \n",
    "This table aggregates some of the data from other existing tables, so it cannot be generated without using some of the previous data, while interpolating additional information. \n",
    "1. Program Cost aggregation from Transactions\n",
    "2. Participant-related transaction count\n",
    "3. Vendor Costs from External Vendor Information\n",
    "4. Unclaimed property such as refunds or grants not climaed\n",
    "5. Investment Portfolio Holding gunding sources that can link to market value or quantity held\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Reporting_Period        Program_Name  Program_Cost  Participants  \\\n",
      "0       2023-06-30     Consulting Fees     860995.10           445   \n",
      "1       2023-06-30   Event Sponsorship    2087755.90           692   \n",
      "2       2023-06-30       Grant Funding     468331.81           243   \n",
      "3       2023-06-30     Interest Income     836576.70           329   \n",
      "4       2023-06-30  Investment Returns    1628995.07           910   \n",
      "\n",
      "   Successful_Outcomes Program_ID  Budget_Allocation  Budget_Utilization_Rate  \\\n",
      "0                  425     PRG001         1212350.29                    71.02   \n",
      "1                  652     PRG002         2673164.80                    78.10   \n",
      "2                  233     PRG003          566114.29                    82.73   \n",
      "3                  187     PRG004         1166001.48                    71.75   \n",
      "4                  561     PRG005         2141855.60                    76.06   \n",
      "\n",
      "  Completion_Rate  Participant_Satisfaction  Cost_per_Participant  \\\n",
      "0          95.51%                       3.5               1934.82   \n",
      "1          94.22%                       3.6               3016.99   \n",
      "2          95.88%                       4.3               1927.29   \n",
      "3          56.84%                       4.6               2542.79   \n",
      "4          61.65%                       4.0               1790.10   \n",
      "\n",
      "   Cost_per_Successful_Outcome On_Time_Completion  \n",
      "0                      2025.87                 No  \n",
      "1                      3202.08                Yes  \n",
      "2                      2010.01                 No  \n",
      "3                      4473.67                 No  \n",
      "4                      2903.73                Yes  \n"
     ]
    }
   ],
   "source": [
    "# Ensure Transaction_Date is in datetime format\n",
    "transactions_clean_df['Transaction_Date'] = pd.to_datetime(transactions_clean_df['Transaction_Date'], errors='coerce')\n",
    "\n",
    "# Drop rows with invalid or missing Transaction_Date\n",
    "transactions_clean_df = transactions_clean_df.dropna(subset=['Transaction_Date'])\n",
    "\n",
    "# Filter transactions related to programs\n",
    "program_related_transactions = transactions_clean_df[\n",
    "    (transactions_clean_df['Description'] == 'Program Fees') |\n",
    "    (transactions_clean_df['Department'] == 'Community Programs')\n",
    "]\n",
    "\n",
    "# Aggregate program costs by reporting period (quarterly)\n",
    "program_metrics = program_related_transactions.groupby(\n",
    "    [pd.Grouper(key='Transaction_Date', freq='QE'), 'Description']\n",
    ").agg(\n",
    "    Program_Cost=('Transaction_Amount', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Rename columns for clarity\n",
    "program_metrics.rename(columns={'Description': 'Program_Name', 'Transaction_Date': 'Reporting_Period'}, inplace=True)\n",
    "\n",
    "# Generate random participants for each program\n",
    "program_metrics['Participants'] = np.random.randint(50, 1000, size=len(program_metrics))\n",
    "\n",
    "# Generate successful outcomes as a subset of participants\n",
    "program_metrics['Successful_Outcomes'] = program_metrics['Participants'].apply(lambda x: np.random.randint(int(x * 0.5), x + 1))\n",
    "\n",
    "# Add unique Program IDs\n",
    "program_metrics['Program_ID'] = [f'PRG{str(i).zfill(3)}' for i in range(1, len(program_metrics) + 1)]\n",
    "\n",
    "# Add additional metrics\n",
    "\n",
    "# Example budget allocations (randomized for demonstration purposes)\n",
    "program_metrics['Budget_Allocation'] = np.round(program_metrics['Program_Cost'] * np.random.uniform(1.1, 1.5, size=len(program_metrics)), 2)\n",
    "\n",
    "# Calculate Budget Utilization Rate\n",
    "program_metrics['Budget_Utilization_Rate'] = (program_metrics['Program_Cost'] / program_metrics['Budget_Allocation'] * 100).round(2)\n",
    "\n",
    "# Calculate Completion Rate\n",
    "program_metrics['Completion_Rate'] = (program_metrics['Successful_Outcomes'] / program_metrics['Participants'] * 100).round(2).astype(str) + '%'\n",
    "\n",
    "# Generate Participant Satisfaction scores (randomized between 3.5 and 5.0)\n",
    "program_metrics['Participant_Satisfaction'] = [round(np.random.uniform(3.5, 5.0), 1) for _ in range(len(program_metrics))]\n",
    "\n",
    "# Calculate Cost per Participant\n",
    "program_metrics['Cost_per_Participant'] = (program_metrics['Program_Cost'] / program_metrics['Participants']).round(2)\n",
    "\n",
    "# Calculate Cost per Successful Outcome\n",
    "program_metrics['Cost_per_Successful_Outcome'] = (program_metrics['Program_Cost'] / program_metrics['Successful_Outcomes']).round(2)\n",
    "\n",
    "# Add On-Time Completion status (randomized for demonstration purposes)\n",
    "program_metrics['On_Time_Completion'] = np.random.choice(['Yes', 'No'], size=len(program_metrics))\n",
    "\n",
    "# Preview the updated Program Performance Metrics Table\n",
    "print(program_metrics.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lovely. Guess what we're going to do with this masterpiece? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Reporting_Period           Program_Name Program_Cost  Participants  \\\n",
      "0   2024-06-30 00:00:00            Maintenance   1999755.17         225.0   \n",
      "1   2024-03-31 00:00:00        Consulting Fees    2238505.6         442.0   \n",
      "2   2023-12-31 00:00:00  Software Subscription   1282043.08         920.0   \n",
      "3   2024-03-31 00:00:00            Maintenance   1930857.38         683.0   \n",
      "4   2023-12-31 00:00:00          Reimbursement    720991.95           NaN   \n",
      "5   2025-03-31 00:00:00           Program Fees   5207713.89         693.0   \n",
      "6   2024-09-30 00:00:00          Grant Funding    6228566.0         587.0   \n",
      "7   2023-12-31 00:00:00        Consulting Fees   1050555.22         780.0   \n",
      "8   2023-09-30 00:00:00        Interest Income    1286578.1          55.0   \n",
      "9   2023-09-30 00:00:00        Office Supplies   1943144.84         971.0   \n",
      "10  2025-06-30 00:00:00        Office Supplies    691059.44         409.0   \n",
      "11  2024-03-31 00:00:00        Office Supplies   1808022.02         898.0   \n",
      "12  2023-09-30 00:00:00        Consulting Fees   2323707.28         669.0   \n",
      "13  2023-06-30 00:00:00        Interest Income     836576.7         329.0   \n",
      "14  2023-09-30 00:00:00      Event Sponsorship   1413899.93         957.0   \n",
      "15  2025-06-30 00:00:00        Interest Income    188263.79         464.0   \n",
      "16  2023-06-30 00:00:00          Grant Funding    468331.81         243.0   \n",
      "17  2025-03-31 00:00:00        Interest Income    238006.48         331.0   \n",
      "18  2025-03-31 00:00:00        Travel Expenses    1304883.1         275.0   \n",
      "19  2023-09-30 00:00:00          Grant Funding    411353.26         808.0   \n",
      "\n",
      "    Successful_Outcomes Program_ID  Budget_Allocation  \\\n",
      "0                   131     PRG050         2972514.95   \n",
      "1                   397     PRG034         2584421.16   \n",
      "2                   518     PRG032         1826517.57   \n",
      "3                   572     PRG039         2586886.19   \n",
      "4                   416     PRG031          970445.27   \n",
      "5                   388     PRG085         5745058.86   \n",
      "6                   501     PRG058          924843.40   \n",
      "7                   414     PRG023         1163146.52   \n",
      "8                    50     PRG015                NaN   \n",
      "9                   854     PRG018         2221481.83   \n",
      "10                  357     PRG095          998665.82   \n",
      "11                  604     PRG040         2048762.57   \n",
      "12                  609     PRG012         2699769.71   \n",
      "13                  187     PRG004         1166001.48   \n",
      "14                  922     PRG013         1632554.20   \n",
      "15                  311     PRG092          234454.23   \n",
      "16                  233     PRG003          566114.29   \n",
      "17                  296     PRG081          354343.81   \n",
      "18                  220     PRG088         1504238.93   \n",
      "19                  674     PRG014          498178.58   \n",
      "\n",
      "    Budget_Utilization_Rate Completion_Rate  Participant_Satisfaction  \\\n",
      "0                     67.27          58.22%                       3.7   \n",
      "1                     86.62          89.82%                       4.0   \n",
      "2                     70.19           56.3%                       4.4   \n",
      "3                     74.64          83.75%                       4.0   \n",
      "4                     74.29          87.39%                       4.3   \n",
      "5                     90.65          55.99%                       4.7   \n",
      "6                     67.35          85.35%                       3.9   \n",
      "7                     90.32          53.08%                       4.2   \n",
      "8                     69.48          90.91%                       3.9   \n",
      "9                     87.47          87.95%                       3.6   \n",
      "10                    69.20          87.29%                       4.8   \n",
      "11                    88.25          67.26%                       4.0   \n",
      "12                    86.07          91.03%                       3.8   \n",
      "13                    71.75          56.84%                       4.6   \n",
      "14                    86.61          96.34%                       3.7   \n",
      "15                    80.30          67.03%                       4.9   \n",
      "16                    82.73          95.88%                       4.3   \n",
      "17                    67.17          89.43%                       4.8   \n",
      "18                    86.75           80.0%                       4.4   \n",
      "19                    82.57          83.42%                       3.6   \n",
      "\n",
      "    Cost_per_Participant  Cost_per_Successful_Outcome On_Time_Completion  \n",
      "0                8887.80                     15265.31                 No  \n",
      "1                5064.49                      5638.55                 No  \n",
      "2                1393.53                      2474.99                 No  \n",
      "3                2827.02                      3375.62                 No  \n",
      "4                1514.69                      1733.15                 No  \n",
      "5                7514.74                     13421.94                Yes  \n",
      "6                1061.08                      1243.23                 No  \n",
      "7                1346.87                      2537.57                 No  \n",
      "8               23392.33                     25731.56                 No  \n",
      "9                2001.18                      2275.35                Yes  \n",
      "10               1689.63                      1935.74                 No  \n",
      "11               2013.39                      2993.41                Yes  \n",
      "12               3473.40                      3815.61                 No  \n",
      "13               2542.79                      4473.67                 No  \n",
      "14               1477.43                      1533.51                Yes  \n",
      "15                405.74                       605.35                Yes  \n",
      "16               1927.29                      2010.01                 No  \n",
      "17                719.05                       804.08                 No  \n",
      "18               4745.03                      5931.29                Yes  \n",
      "19                509.10                       610.32                 No  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\justi\\AppData\\Local\\Temp\\ipykernel_1724\\2675814484.py:35: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['2023-09-30 INVALID' '2024-03-31 INVALID']' has dtype incompatible with datetime64[ns], please explicitly cast to a compatible dtype first.\n",
      "  program_metrics.loc[format_error_indices, 'Reporting_Period'] = program_metrics.loc[\n",
      "C:\\Users\\justi\\AppData\\Local\\Temp\\ipykernel_1724\\2675814484.py:48: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['1169852.0899999999' '468331.81']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  program_metrics.loc[type_error_indices, 'Program_Cost'] = program_metrics.loc[\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# 1. Introduce Logical Inconsistencies\n",
    "# Set Successful_Outcomes greater than Participants\n",
    "logical_error_indices = program_metrics.sample(n=num_nulls, random_state=random_state).index\n",
    "program_metrics.loc[logical_error_indices, 'Successful_Outcomes'] = program_metrics.loc[\n",
    "    logical_error_indices, 'Participants'\n",
    "].apply(lambda x: x + random.randint(1, 50) if x is not None else None)\n",
    "\n",
    "# Set Budget_Utilization_Rate above 100% or below 0%\n",
    "budget_error_indices = program_metrics.sample(n=num_nulls, random_state=random_state_alt).index\n",
    "program_metrics.loc[budget_error_indices, 'Budget_Utilization_Rate'] = program_metrics.loc[\n",
    "    budget_error_indices, 'Budget_Utilization_Rate'\n",
    "].apply(lambda x: x * random.choice([-1, 2]))\n",
    "\n",
    "# Assign On_Time_Completion as \"Yes\" for programs with no participants\n",
    "completion_error_indices = program_metrics[program_metrics['Participants'].isnull()].index\n",
    "program_metrics.loc[completion_error_indices, 'On_Time_Completion'] = 'Yes'\n",
    "\n",
    "# 2. Introduce Outliers\n",
    "# Add extreme values to Program_Cost\n",
    "outlier_indices = program_metrics.sample(n=num_nulls, random_state=random_state).index\n",
    "program_metrics.loc[outlier_indices, 'Program_Cost'] = program_metrics.loc[\n",
    "    outlier_indices, 'Program_Cost'\n",
    "] * random.choice([10, 0.01])\n",
    "\n",
    "# 3. Introduce Duplicate Rows\n",
    "num_duplicates = int(len(program_metrics) * 0.02)  # 2% duplicates\n",
    "duplicates_df = program_metrics.sample(n=num_duplicates, random_state=random_state)\n",
    "program_metrics = pd.concat([program_metrics, duplicates_df], ignore_index=True)\n",
    "\n",
    "# 4. Introduce Inconsistent Formats\n",
    "# Convert some Reporting_Period values to strings\n",
    "format_error_indices = program_metrics.sample(n=num_nulls, random_state=random_state).index\n",
    "program_metrics.loc[format_error_indices, 'Reporting_Period'] = program_metrics.loc[\n",
    "    format_error_indices, 'Reporting_Period'\n",
    "].astype(str) + ' INVALID'\n",
    "\n",
    "# Modify Program_Name to introduce inconsistent capitalization\n",
    "name_error_indices = program_metrics.sample(n=num_nulls, random_state=random_state).index\n",
    "program_metrics.loc[name_error_indices, 'Program_Name'] = program_metrics.loc[\n",
    "    name_error_indices, 'Program_Name'\n",
    "].apply(lambda x: x.lower() if random.random() > 0.5 else x.upper())\n",
    "\n",
    "# 5. Introduce Incorrect Data Types\n",
    "# Convert numeric columns to strings\n",
    "type_error_indices = program_metrics.sample(n=num_nulls, random_state=random_state).index\n",
    "program_metrics.loc[type_error_indices, 'Program_Cost'] = program_metrics.loc[\n",
    "    type_error_indices, 'Program_Cost'\n",
    "].astype(str)\n",
    "\n",
    "# 6. Introduce Missing Values (Nulls)\n",
    "num_nulls = int(len(program_metrics) * 0.05)  # 5% nulls\n",
    "null_indices_participants = program_metrics.sample(n=num_nulls, random_state=random_state).index\n",
    "null_indices_budget = program_metrics.sample(n=num_nulls, random_state=random_state_alt).index\n",
    "\n",
    "program_metrics.loc[null_indices_participants, 'Participants'] = None\n",
    "program_metrics.loc[null_indices_budget, 'Budget_Allocation'] = None\n",
    "\n",
    "# Shuffle the DataFrame to mix errors naturally\n",
    "program_metrics = program_metrics.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "\n",
    "# Preview the table with errors\n",
    "print(program_metrics.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Program Performance Metrics Table has been mutilated and regurgitated as 'program_performance_metrics.xlsx'.\n"
     ]
    }
   ],
   "source": [
    "# Save to excel, frown, cry... just let it all out.\n",
    "program_metrics.to_excel('program_performance_metrics.xlsx', index=False, sheet_name='Program Metrics')\n",
    "\n",
    "# Confirm the file was saved in such a horrid manner\n",
    "print(\"Program Performance Metrics Table has been mutilated and regurgitated as 'program_performance_metrics.xlsx'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, alright, alright! \n",
    "The last step is to just verify that we have all of these tables saved in the crap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying External Vendor Information (external_vendor_information.csv):\n",
      "This crappy table exists. Lucky you. \n",
      "\n",
      "Verifying Portfolio Holdings (portfolio_holdings.csv):\n",
      "This crappy table exists. Lucky you. \n",
      "\n",
      "Verifying Transactions (transactions.csv):\n",
      "This crappy table exists. Lucky you. \n",
      "\n",
      "Verifying Unclaimed Property Records (unclaimed_property_records.csv):\n",
      "This crappy table exists. Lucky you. \n",
      "\n",
      "Verifying Program Performance Metrics (program_performance_metrics.xlsx):\n",
      "This crappy table exists. Lucky you. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File paths\n",
    "files = {\n",
    "    \"External Vendor Information\": \"external_vendor_information.csv\"\n",
    "    , \"Portfolio Holdings\": \"portfolio_holdings.csv\"\n",
    "    , \"Transactions\": \"transactions.csv\"\n",
    "    , \"Unclaimed Property Records\": \"unclaimed_property_records.csv\"\n",
    "    , \"Program Performance Metrics\": \"program_performance_metrics.xlsx\"\n",
    "}\n",
    "\n",
    "# Loop through each file, read it, and print the head\n",
    "for table_name, file_path in files.items():\n",
    "    print(f\"Verifying {table_name} ({file_path}):\")\n",
    "    try:\n",
    "        # Check file extension to determine how to read the file\n",
    "        if file_path.endswith('.csv'):\n",
    "            df = pd.read_csv(file_path)\n",
    "        elif file_path.endswith('.xlsx'):\n",
    "            df = pd.read_excel(file_path)\n",
    "        else:\n",
    "            print(f\"Unsupported file format for {file_path}. Skipping...\\n\")\n",
    "            continue\n",
    "        \n",
    "        # Print the first few rows of the DataFrame\n",
    "        if not df.empty:\n",
    "            print(\"This crappy table exists. Lucky you.\", \"\\n\")\n",
    "        else:\n",
    "            print(f\"The file {file_path} is empty.\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error was squeezed out while reading {file_path}: {e}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "treasury",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
