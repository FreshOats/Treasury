{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Synthetic Data for Treasury Analytics\n",
    "\n",
    "1. Transactions Table (Largest Dataset, 10000 records)\n",
    "Reasoning: Financial transactions typically occur frequently and continuously, resulting in a large volume of records. This dataset would naturally be the largest, as it captures daily operational expenses, revenues, vendor payments, and other financial activities. Transactional data often accumulates rapidly, making it the most extensive dataset.\n",
    "\n",
    "2. Investment Portfolio Holdings (Second Largest, 250 records)\n",
    "Reasoning: Investment portfolios usually contain numerous securities, each with multiple transactions (buys, sells, dividends, interest payments). While not as frequent as daily operational transactions, investment holdings still generate substantial data, especially if historical records and periodic valuations are included.\n",
    "\n",
    "3. External Vendor Information (~200 records)\n",
    "Reasoning: This dimension table focuses on vendors with formal contracts or significant financial relationships with the organization.\n",
    "\n",
    "\n",
    "4. Unclaimed Property Records (~200 records)\n",
    "Reasoning: Unclaimed property records accumulate steadily but at a slower rate compared to financial transactions or investment activities. These records are typically updated periodically (monthly, quarterly, or annually), resulting in fewer records than the first two datasets.\n",
    "\n",
    "5. Program Performance Metrics (Smallest Dataset, 20-50 records)\n",
    "Reasoning: Program performance metrics are usually aggregated data points collected periodically (monthly, quarterly, or annually). They summarize program outcomes, participant counts, and costs, resulting in fewer records compared to transactional or investment data.\n",
    "Recommended Dataset Sizes (for your dummy data project). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from faker import Faker\n",
    "import random\n",
    "\n",
    "fake = Faker()\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(91942)\n",
    "random_state = random.seed(91942)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Dataset 1: ISTO Financial Transactions\n",
    "Purpose: Analyze spending patterns, identify cost-saving opportunities, and visualize financial trends.\n",
    "Table Type: Fact table\n",
    "\n",
    "**Schema:**\n",
    "- Transaction_ID (unique identifier) VARCHAR\n",
    "- Transaction_Date DATETIME\n",
    "- Transaction_Amount FLOAT\n",
    "- Transaction_Type (Expense, Revenue) VARCHAR\n",
    "- Department (e.g., Operations, Investments, Community Programs) VARCHAR\n",
    "- Vendor_Name VARCHAR\n",
    "- Description VARCHAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Transaction_ID Transaction_Date  Transaction_Amount Transaction_Type  \\\n",
      "0    TXN00000001       2023-11-17            57080.47          Expense   \n",
      "1    TXN00000002       2024-07-09           100948.94          Expense   \n",
      "2    TXN00000003       2024-11-29            82489.87          Expense   \n",
      "3    TXN00000004       2024-02-18           149280.56          Revenue   \n",
      "4    TXN00000005       2024-07-18            34606.19          Expense   \n",
      "\n",
      "    Department                    Vendor_Name      Description  \n",
      "0  Investments  Holloway, Archer and Espinoza      Maintenance  \n",
      "1   Compliance                      Green Inc  Office Supplies  \n",
      "2   Operations                   Schwartz Inc  Travel Expenses  \n",
      "3  Investments                Rodriguez Group    Grant Funding  \n",
      "4           IT    Smith, Parsons and Williams  Office Supplies  \n",
      "Total transactions: 10000\n",
      "Unique vendors: 954\n",
      "Vendors appearing more than once: 667\n"
     ]
    }
   ],
   "source": [
    "# Number of rows\n",
    "num_rows = 10000\n",
    "num_unique_vendors = 1159 \n",
    "\n",
    "# Generate Transaction IDs (formatted as TXN00000001)\n",
    "transaction_ids = [f'TXN{str(i).zfill(8)}' for i in range(1, num_rows + 1)]\n",
    "\n",
    "# Generate random dates within a realistic range (e.g., past 2 years)\n",
    "transaction_dates = [fake.date_between(start_date='-2y', end_date='today') for _ in range(num_rows)]\n",
    "\n",
    "# Generate transaction amounts (expenses and revenues)\n",
    "transaction_amounts = np.round(np.random.uniform(50, 150000, num_rows), 2)\n",
    "\n",
    "# Randomly assign transaction types\n",
    "transaction_types = np.random.choice(['Expense', 'Revenue'], size=num_rows, p=[0.7, 0.3])\n",
    "\n",
    "# Departments\n",
    "departments = ['Operations', 'Investments', 'Community Programs', 'Administration', 'IT', 'Compliance']\n",
    "department_choices = np.random.choice(departments, size=num_rows)\n",
    "\n",
    "# Vendor names (using Faker), limiting to lower unique vendors\n",
    "unique_vendors = [fake.unique.company() for _ in range(num_unique_vendors)]\n",
    "\n",
    "# Create a realistic distribution of vendor frequencies\n",
    "vendor_probabilities = np.random.zipf(a=2, size=num_unique_vendors)\n",
    "vendor_probabilities = vendor_probabilities / vendor_probabilities.sum()\n",
    "\n",
    "# Assign vendors to transactions based on probabilities\n",
    "vendor_names = np.random.choice(unique_vendors, size=num_rows, p=vendor_probabilities)\n",
    "\n",
    "# Transaction descriptions (simple examples)\n",
    "descriptions_expense = ['Office Supplies', 'Consulting Fees', 'Software Subscription', 'Event Sponsorship', 'Maintenance', 'Travel Expenses']\n",
    "descriptions_revenue = ['Interest Income', 'Investment Returns', 'Program Fees', 'Grant Funding', 'Reimbursement']\n",
    "\n",
    "descriptions = [\n",
    "    random.choice(descriptions_expense) if t == 'Expense' else random.choice(descriptions_revenue)\n",
    "    for t in transaction_types\n",
    "]\n",
    "\n",
    "# Assemble DataFrame\n",
    "transactions_df = pd.DataFrame({\n",
    "    'Transaction_ID': transaction_ids,\n",
    "    'Transaction_Date': transaction_dates,\n",
    "    'Transaction_Amount': transaction_amounts,\n",
    "    'Transaction_Type': transaction_types,\n",
    "    'Department': department_choices,\n",
    "    'Vendor_Name': vendor_names,\n",
    "    'Description': descriptions\n",
    "})\n",
    "\n",
    "# Preview the dataset\n",
    "print(transactions_df.head())\n",
    "print(\"Total transactions:\", len(transactions_df))\n",
    "print(\"Unique vendors:\", transactions_df.Vendor_Name.nunique())\n",
    "print(\"Vendors appearing more than once:\", (transactions_df.Vendor_Name.value_counts() > 1).sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has created a perfect dataframe with no errors or duplicates. This is unrealistic, so let's shake it up.\n",
    "\n",
    "1. Introduce dupes\n",
    "2. Add incorrect data types\n",
    "3. Add logical inconsistencies\n",
    "4. Give it some outliers!\n",
    "5. Mess up the vendor names including INC\n",
    "6. Remove a few IDs\n",
    "7. Spread some NaN love throughout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Transaction_ID            Transaction_Date  Transaction_Amount  \\\n",
      "0    TXN00003763                  2023-11-19            49388.52   \n",
      "1    TXN00007941  2025-09-28 15:42:22.372415            52924.48   \n",
      "2    TXN00007395                  2024-02-08           140902.59   \n",
      "3    TXN00009310                  2024-01-09            60473.14   \n",
      "4    TXN00009084                  2025-03-10           137488.96   \n",
      "\n",
      "  Transaction_Type      Department                    Vendor_Name  \\\n",
      "0          Expense  Administration  Holloway, Archer and Espinoza   \n",
      "1          Expense      Compliance                    Moore Group   \n",
      "2          Expense  Administration          Ponce, Carr and Brown   \n",
      "3          Expense      Compliance                       Diaz Inc   \n",
      "4          Expense  Administration    Sanders, Henderson and Reed   \n",
      "\n",
      "             Description  \n",
      "0  Software Subscription  \n",
      "1        Consulting Fees  \n",
      "2        Office Supplies  \n",
      "3        Office Supplies  \n",
      "4      Event Sponsorship  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\justi\\AppData\\Local\\Temp\\ipykernel_4372\\2656578920.py:19: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['103538.4' '71506.15' '120225.49' '129209.59' '69280.12' '100484.86'\n",
      " '82174.83' '114616.42' '55552.2' '135823.58' '52923.34' '62042.56'\n",
      " '120506.57' '79590.86' '123237.36' '20528.35' '35164.55' '116512.55'\n",
      " '138830.93' '133164.12' '141469.84' '144046.54' '32740.77' '93226.17'\n",
      " '138094.14' '101212.61' '44310.88' '10794.66' '32747.58' '145253.56'\n",
      " '6716.69' '124463.3' '121659.2' '100058.6' '114127.4' '40838.84'\n",
      " '124203.3' '43014.97' '110460.07' '76922.37' '110888.29' '142715.75'\n",
      " '78947.16' '71146.91' '65448.27' '10842.02' '71859.01' '137943.32'\n",
      " '105404.39' '48441.65' '26752.73' '98718.03' '97671.68' '126678.32'\n",
      " '85428.61' '141451.76' '126947.52' '67966.94' '110043.76' '27480.37'\n",
      " '143568.09' '126267.27' '91074.24' '12732.57' '147709.91' '106504.73'\n",
      " '131184.36' '119501.58' '70779.29' '144606.92' '28477.3' '39193.91'\n",
      " '2936.43' '66219.16' '95026.14' '13196.35' '10666.3' '149190.28'\n",
      " '93285.37' '143967.15' '30004.57' '12659.6' '57597.21' '47684.48'\n",
      " '58539.69' '53962.98' '37755.09' '125014.3' '52827.6' '79804.19'\n",
      " '44292.66' '128304.24' '675.13' '64992.37' '83883.21' '87504.83'\n",
      " '28242.64' '109283.16' '44569.81' '49227.38' '73793.15' '66758.74'\n",
      " '33005.95' '127947.32' '141845.85' '89208.44' '98210.59' '99843.73'\n",
      " '13632.09' '117556.54' '123569.35' '46391.97' '83317.16' '131446.53'\n",
      " '12253.96' '131395.9' '110722.13' '73531.21' '97230.24' '58914.35'\n",
      " '45175.29' '28406.54' '34759.87' '103508.31' '49589.3' '147916.28'\n",
      " '85549.95' '114417.01' '92965.54' '19520.96' '78716.58' '86414.23'\n",
      " '63925.59' '83830.83' '139771.51' '83462.14' '71602.97' '119145.13'\n",
      " '141723.16' '7752.64' '136609.91' '115930.17' '80920.75' '121181.74'\n",
      " '43711.08' '121479.78' '76196.8' '99514.95' '51250.45' '62568.36'\n",
      " '86349.94' '64997.55' '123567.97' '93866.73' '135701.85' '47430.31'\n",
      " '30103.51' '124179.53' '141255.99' '69994.11' '65162.24' '88137.72'\n",
      " '78123.5' '110264.52' '76247.35' '91298.41' '147662.05' '95351.67'\n",
      " '135330.94' '330.08' '64355.59' '96103.89' '66461.26' '106527.57'\n",
      " '50154.04' '93809.08' '103029.18' '10529.39' '2355.35' '44544.96'\n",
      " '64449.98' '7177.01' '28788.4' '22510.56' '65043.92' '82617.41'\n",
      " '140712.12' '110609.52' '138225.98' '129964.04' '31550.47' '3392.96'\n",
      " '42123.0' '130798.29' '56171.7' '82611.86' '123601.96' '106855.6'\n",
      " '108231.23' '98660.26' '111559.38' '147416.97' '113586.49' '114902.75']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  transactions_df.loc[type_error_indices, 'Transaction_Amount'] = transactions_df.loc[type_error_indices, 'Transaction_Amount'].astype(str)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "np.random.seed(91942)\n",
    "random_state = random.seed(91942)\n",
    "random_state_alt = random.seed(24919)\n",
    "\n",
    "transactions_clean_df = copy.deepcopy(transactions_df)\n",
    "\n",
    "# 1. Introduce duplicate rows\n",
    "num_duplicates = int(len(transactions_df) * 0.02)  # 2% duplicates\n",
    "duplicates_df = transactions_df.sample(n=num_duplicates, random_state=random_state)\n",
    "transactions_df = pd.concat([transactions_df, duplicates_df], ignore_index=True)\n",
    "\n",
    "# 2. Introduce incorrect data types\n",
    "num_type_errors = int(len(transactions_df) * 0.02)  # 2% type errors\n",
    "type_error_indices = transactions_df.sample(n=num_type_errors, random_state=random_state).index\n",
    "transactions_df.loc[type_error_indices, 'Transaction_Amount'] = transactions_df.loc[type_error_indices, 'Transaction_Amount'].astype(str)\n",
    "\n",
    "date_error_indices = transactions_df.sample(n=num_type_errors, random_state=random_state_alt).index\n",
    "transactions_df.loc[date_error_indices, 'Transaction_Date'] = transactions_df.loc[date_error_indices, 'Transaction_Date'].astype(str) + ' INVALID'\n",
    "\n",
    "# 3. Introduce logical inconsistencies\n",
    "expense_indices = transactions_df[transactions_df['Transaction_Type'] == 'Expense'].sample(n=num_type_errors, random_state=random_state).index\n",
    "transactions_df.loc[expense_indices, 'Transaction_Amount'] *= -1  # Negative amounts for expenses\n",
    "\n",
    "invalid_department_indices = transactions_df.sample(n=num_type_errors, random_state=random_state_alt).index\n",
    "transactions_df.loc[invalid_department_indices, 'Department'] = 'InvalidDept'\n",
    "\n",
    "# 4. Introduce outliers\n",
    "# Ensure 'Transaction_Amount' is numeric\n",
    "transactions_df['Transaction_Amount'] = pd.to_numeric(transactions_df['Transaction_Amount'], errors='coerce')\n",
    "outlier_indices = transactions_df.sample(n=num_type_errors, random_state=random_state).index\n",
    "\n",
    "# Generate outliers using a log-normal distribution\n",
    "outlier_values = np.random.lognormal(mean=10, sigma=2, size=num_type_errors)\n",
    "transactions_df.loc[outlier_indices, 'Transaction_Amount'] = outlier_values\n",
    "\n",
    "future_date_indices = transactions_df.sample(n=num_type_errors, random_state=random_state_alt).index\n",
    "transactions_df.loc[future_date_indices, 'Transaction_Date'] = pd.Timestamp('today') + pd.to_timedelta(np.random.randint(30, 365, size=num_type_errors), unit='d')\n",
    "\n",
    "# 5. Introduce inconsistent vendor names\n",
    "vendor_indices = transactions_df.sample(n=num_type_errors, random_state=random_state).index\n",
    "transactions_df.loc[vendor_indices, 'Vendor_Name'] = transactions_df.loc[vendor_indices, 'Vendor_Name'].apply(\n",
    "    lambda x: x.replace('Inc.', 'Incorporated') if 'Inc.' in x else x + ' Inc.'\n",
    ")\n",
    "\n",
    "# 6. Introduce missing or invalid Transaction IDs\n",
    "missing_id_indices = transactions_df.sample(n=num_type_errors, random_state=random_state).index\n",
    "transactions_df.loc[missing_id_indices, 'Transaction_ID'] = np.nan\n",
    "\n",
    "# 7. Introduce missing values (nulls) - needs to be last so it doesn't break the multiplication code\n",
    "num_nulls = int(len(transactions_df) * 0.03)  # 3% nulls\n",
    "null_indices_amount = transactions_df.sample(n=num_nulls, random_state=random_state).index\n",
    "null_indices_department = transactions_df.sample(n=num_nulls, random_state=random_state_alt).index\n",
    "transactions_df.loc[null_indices_amount, 'Transaction_Amount'] = np.nan\n",
    "transactions_df.loc[null_indices_department, 'Department'] = np.nan\n",
    "\n",
    "# Shuffle the DataFrame to mix errors naturally\n",
    "transactions_df = transactions_df.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "\n",
    "# Check the resulting DataFrame\n",
    "print(transactions_df.head())\n",
    "# print(transactions_df.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique vendors: 1041\n",
      "Number of duplicated vendors: 9159\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of unique vendors:\", len(transactions_df.Vendor_Name.unique()))\n",
    "print(\"Number of duplicated vendors:\", transactions_df.Vendor_Name.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Transaction_ID', 'Transaction_Date', 'Transaction_Amount',\n",
      "       'Transaction_Type', 'Department', 'Vendor_Name', 'Description'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(transactions_df.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, save the data in a horrible data format: csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV and cringe\n",
    "transactions_df.to_csv('transactions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Dataset #2 Investment Portfolio Holdings\n",
    "Purpose: Analyze ISTO's investment portfolio, diversification, and risk management.\n",
    "Table Type: Dimension\n",
    "\n",
    "**Schema:**\n",
    "- Holding_ID VARCHAR\n",
    "- Security_Name VARCHAR\n",
    "- Security_Type VARCHAR\n",
    "- Quantity_Held INT\n",
    "- Market_Value FLOAT\n",
    "- Acquisition_Date DATETIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Holding_ID                  Security_Name Security_Type  Quantity_Held  \\\n",
      "0   HLD00001        Rogers, Lane and Miller         Stock            544   \n",
      "1   HLD00002  Jackson, Blankenship and Hart          Bond           1106   \n",
      "2   HLD00003                  Hodge-Hawkins          Bond           1841   \n",
      "3   HLD00004        Scott, Chapman and Buck   Mutual Fund           1427   \n",
      "4   HLD00005       Rojas, Thomas and Zavala         Stock            805   \n",
      "\n",
      "   Market_Price_Per_Unit  Total_Market_Value Acquisition_Date  \n",
      "0                 493.30           268355.20       2022-09-07  \n",
      "1                  44.65            49382.90       2022-11-17  \n",
      "2                  53.83            99101.03       2020-10-04  \n",
      "3                 424.49           605747.23       2025-01-18  \n",
      "4                  96.97            78060.85       2020-07-20  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from faker import Faker\n",
    "\n",
    "fake = Faker()\n",
    "np.random.seed(91942)\n",
    "\n",
    "# Number of holdings\n",
    "num_holdings = 257\n",
    "\n",
    "# Generate Holding IDs\n",
    "holding_ids = [f'HLD{str(i).zfill(5)}' for i in range(1, num_holdings + 1)]\n",
    "\n",
    "# Generate realistic security names (e.g., company names)\n",
    "security_names = [fake.unique.company() for _ in range(num_holdings)]\n",
    "\n",
    "# Security types distribution\n",
    "security_types = np.random.choice(\n",
    "    ['Stock', 'Bond', 'ETF', 'Mutual Fund'],\n",
    "    size=num_holdings,\n",
    "    p=[0.5, 0.2, 0.2, 0.1]\n",
    ")\n",
    "\n",
    "# Quantity held (realistic quantities)\n",
    "quantity_held = np.random.randint(100, 5000, size=num_holdings)\n",
    "\n",
    "# Market value per unit (realistic prices)\n",
    "market_price_per_unit = np.round(np.random.uniform(10, 500, size=num_holdings), 2)\n",
    "\n",
    "# Calculate total market value\n",
    "market_values = np.round(quantity_held * market_price_per_unit, 2)\n",
    "\n",
    "# Acquisition dates (within past 5 years)\n",
    "acquisition_dates = pd.to_datetime([fake.date_between(start_date='-5y', end_date='today') for _ in range(num_holdings)])\n",
    "\n",
    "\n",
    "# Assemble DataFrame\n",
    "portfolio_holdings_df = pd.DataFrame({\n",
    "    'Holding_ID': holding_ids,\n",
    "    'Security_Name': security_names,\n",
    "    'Security_Type': security_types,\n",
    "    'Quantity_Held': quantity_held,\n",
    "    'Market_Price_Per_Unit': market_price_per_unit,\n",
    "    'Total_Market_Value': market_values,\n",
    "    'Acquisition_Date': acquisition_dates\n",
    "})\n",
    "\n",
    "# Preview the dataset\n",
    "print(portfolio_holdings_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Transaction_ID', 'Transaction_Date', 'Transaction_Amount',\n",
      "       'Transaction_Type', 'Department', 'Vendor_Name', 'Description'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(transactions_df.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a perfect dataset, let's make it crappy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Holding_ID             Security_Name Security_Type  Quantity_Held  \\\n",
      "0   HLD00144               Ramirez Ltd         Stock           2908   \n",
      "1   HLD00145             Ball and Sons           ETF           2357   \n",
      "2   HLD00226                 Doyle PLC         Stock           2428   \n",
      "3   HLD00182  Woods, Spencer and Dixon   Mutual Fund           4352   \n",
      "4   HLD00239    Cook, Hill and Salazar          Bond           4716   \n",
      "\n",
      "   Market_Price_Per_Unit  Total_Market_Value Acquisition_Date  \n",
      "0                 157.64           458417.12       2023-06-17  \n",
      "1                 487.14          1148188.98       2023-02-19  \n",
      "2                 201.93           490286.04       2022-10-01  \n",
      "3                 492.02          2141271.04       2022-01-14  \n",
      "4                 426.09          2009440.44       2022-04-08  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from faker import Faker\n",
    "\n",
    "fake = Faker()\n",
    "np.random.seed(91942)\n",
    "random_state=random.seed(91942)\n",
    "random_state_alt = random.seed(24919)\n",
    "\n",
    "# 1. Introduce duplicate holdings with slight variations (3%)\n",
    "num_duplicates = int(len(portfolio_holdings_df) * 0.03)\n",
    "duplicates_df = portfolio_holdings_df.sample(n=num_duplicates, random_state=random_state).copy()\n",
    "\n",
    "duplicates_df['Security_Name'] = duplicates_df['Security_Name'].apply(\n",
    "    lambda x: x + ' Inc.' if 'Inc.' not in x else x.replace('Inc.', 'Incorporated')\n",
    ")\n",
    "duplicates_df['Acquisition_Date'] += pd.to_timedelta(\n",
    "    np.random.randint(-10, 10, size=num_duplicates), unit='d'\n",
    ")\n",
    "\n",
    "# 2. Introduce missing values in 'Market_Price_Per_Unit' (2%)\n",
    "num_nulls = int(len(portfolio_holdings_df) * 0.02)\n",
    "null_indices = portfolio_holdings_df.sample(n=num_nulls, random_state=random_state).index\n",
    "portfolio_holdings_df.loc[null_indices, 'Market_Price_Per_Unit'] = np.nan\n",
    "\n",
    "# 3. Introduce unrealistic market prices (outliers) (2%)\n",
    "num_outliers = int(len(portfolio_holdings_df) * 0.02)\n",
    "outlier_indices = portfolio_holdings_df.sample(n=num_outliers, random_state=random_state_alt).index\n",
    "portfolio_holdings_df.loc[outlier_indices, 'Market_Price_Per_Unit'] *= np.random.choice([0.01, 100], size=num_outliers)\n",
    "portfolio_holdings_df['Total_Market_Value'] = portfolio_holdings_df['Quantity_Held'] * portfolio_holdings_df['Market_Price_Per_Unit']\n",
    "\n",
    "# 4. Introduce incorrect security types (misclassification) (3%)\n",
    "num_misclassified = int(len(portfolio_holdings_df) * 0.03)\n",
    "misclassified_indices = portfolio_holdings_df.sample(n=num_misclassified, random_state=random_state).index\n",
    "portfolio_holdings_df.loc[misclassified_indices, 'Security_Type'] = np.random.choice(['Stock', 'Bond', 'ETF', 'Mutual Fund'], size=num_misclassified)\n",
    "\n",
    "# 5. Introduce future acquisition dates (2%)\n",
    "num_future_dates = int(len(portfolio_holdings_df) * 0.02)\n",
    "future_indices = portfolio_holdings_df.sample(n=num_future_dates, random_state=random_state_alt).index\n",
    "portfolio_holdings_df.loc[future_indices, 'Acquisition_Date'] = pd.Timestamp('today') + pd.to_timedelta(np.random.randint(30, 365, size=num_future_dates), unit='d')\n",
    "\n",
    "# Combine duplicates back into the original DataFrame\n",
    "portfolio_holdings_df = pd.concat([portfolio_holdings_df, duplicates_df], ignore_index=True)\n",
    "\n",
    "# Shuffle the DataFrame to mix errors naturally\n",
    "portfolio_holdings_df = portfolio_holdings_df.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "\n",
    "# Check the resulting DataFrame\n",
    "print(portfolio_holdings_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV and cringe some more\n",
    "portfolio_holdings_df.to_csv('portfolio_holdings.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Dataset #3: External Vendor Information\n",
    "Contains descriptive attributes about the vendors, and provides context for analyzing transactions from the transaction table. \n",
    "\n",
    "Table Type: Dimension\n",
    "**Schema:**\n",
    "- Vendor_ID VARHCAR\n",
    "- Vendor_Name VARCHAR\n",
    "- Service Type VARCHAR\n",
    "- Contract_Start_Date DATETIME\n",
    "- Contract_End_Date DATETIME\n",
    "- Amount_Paid_YTD FLOAT\n",
    "\n",
    "\n",
    "Considerations: \n",
    "Generating this table has a few dependencies: \n",
    "1. The external vendors with contracts should be a subset of the vendors making transactions\n",
    "2. If it is an external vendor with a contract, they should not be making transactions AFTER their contract has ended\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Vendor_ID               Vendor_Name Service_Type Contract_Start_Date  \\\n",
      "0   VND0001            Holder-Roberts   Consulting          2023-10-25   \n",
      "1   VND0002                 Dixon LLC     Supplies          2021-10-15   \n",
      "2   VND0003  Murray, Fuller and Smith  Maintenance          2020-09-01   \n",
      "3   VND0004           Whitehead Group  Maintenance          2022-02-21   \n",
      "4   VND0005           Thompson-Barker     Supplies          2022-03-19   \n",
      "\n",
      "           Contract_End_Date  Amount_Paid_YTD  \n",
      "0 2025-09-30 00:00:00.000000         89484.49  \n",
      "1 2025-02-01 00:00:00.000000         75736.47  \n",
      "2 2025-04-17 15:42:22.835525         44447.81  \n",
      "3 2024-11-04 00:00:00.000000         16022.84  \n",
      "4 2025-04-17 15:42:22.835525         88657.40  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from faker import Faker\n",
    "\n",
    "# Initialize Faker for generating random data\n",
    "fake = Faker()\n",
    "np.random.seed(91942)\n",
    "\n",
    "# Number of vendors to include in the External Vendor Information Table\n",
    "num_vendors = 200\n",
    "\n",
    "# Uses the unique_vendors from the Transactions Table\n",
    "external_vendor_subset = np.random.choice(unique_vendors, size=num_vendors, replace=False)\n",
    "\n",
    "# Generate unique Vendor IDs for the subset\n",
    "vendor_ids = [f'VND{str(i).zfill(4)}' for i in range(1, num_vendors + 1)]\n",
    "\n",
    "# Generate random Service Types\n",
    "service_types = np.random.choice(\n",
    "    ['IT Services', 'Consulting', 'Supplies', 'Financial Services', 'Maintenance'],\n",
    "    size=num_vendors\n",
    ")\n",
    "\n",
    "# Generate random Contract Start Dates (within the past 5 years)\n",
    "contract_start_dates = pd.to_datetime(\n",
    "    [fake.date_between(start_date='-5y', end_date='today') for _ in range(num_vendors)]\n",
    ")\n",
    "\n",
    "# Generate random Contract End Dates (1â€“3 years after the start date)\n",
    "contract_end_dates = contract_start_dates + pd.to_timedelta(\n",
    "    np.random.randint(365, 1095, size=num_vendors), unit='d'\n",
    ")\n",
    "\n",
    "# Simulate Transactions DataFrame to get the latest transaction date for each vendor\n",
    "# Generate transaction dates within the past 2 years\n",
    "transaction_dates = [fake.date_between(start_date='-2y', end_date='today') for _ in range(10000)]\n",
    "\n",
    "# Generate a single Zipf distribution for vendor probabilities\n",
    "zipf_distribution = np.random.zipf(a=2, size=1159)\n",
    "vendor_probabilities = zipf_distribution / zipf_distribution.sum()\n",
    "\n",
    "# Assign vendors to transactions based on the normalized probabilities\n",
    "transaction_vendors = np.random.choice(unique_vendors, size=10000, p=vendor_probabilities)\n",
    "\n",
    "# Create the Transactions DataFrame\n",
    "transactions_vendors_df = pd.DataFrame({\n",
    "    'Transaction_Date': transaction_dates,\n",
    "    'Vendor_Name': transaction_vendors\n",
    "})\n",
    "\n",
    "# Ensure Contract End Dates are after the latest transaction date for each vendor\n",
    "latest_transaction_dates = transactions_vendors_df.groupby('Vendor_Name')['Transaction_Date'].max().apply(pd.Timestamp)\n",
    "contract_end_dates = [\n",
    "    max(latest_transaction_dates.get(vendor, pd.Timestamp('today')), end_date)\n",
    "    for vendor, end_date in zip(external_vendor_subset, contract_end_dates)\n",
    "]\n",
    "\n",
    "# Generate random Amount Paid Year-to-Date (YTD)\n",
    "amount_paid_ytd = np.round(np.random.uniform(5000, 200000, size=num_vendors), 2)\n",
    "\n",
    "# Create the External Vendor Information DataFrame\n",
    "external_vendor_df = pd.DataFrame({\n",
    "    'Vendor_ID': vendor_ids,\n",
    "    'Vendor_Name': external_vendor_subset,\n",
    "    'Service_Type': service_types,\n",
    "    'Contract_Start_Date': contract_start_dates,\n",
    "    'Contract_End_Date': contract_end_dates,\n",
    "    'Amount_Paid_YTD': amount_paid_ytd\n",
    "})\n",
    "\n",
    "# Preview the table\n",
    "print(external_vendor_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we have perfect data that we need to ruin by \n",
    "Adding duplicate rows\n",
    "Inconsistent date formats\n",
    "Outliers\n",
    "Logical inconsistencies\n",
    "Inconsistent naming conventions\n",
    "missing IDs\n",
    "Null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Vendor_ID                      Vendor_Name        Service_Type  \\\n",
      "0    VND0001                   Holder-Roberts          Consulting   \n",
      "1    VND0002                        Dixon LLC            Supplies   \n",
      "2    VND0003         Murray, Fuller and Smith         Maintenance   \n",
      "3    VND0004                  Whitehead Group         Maintenance   \n",
      "4    VND0005                  Thompson-Barker            Supplies   \n",
      "5    VND0006           Mata, Koch and Barrett         Maintenance   \n",
      "6    VND0007         Pierce, Mendoza and Hill         Maintenance   \n",
      "7    VND0008       Hughes, Thomas and Schmidt  Financial Services   \n",
      "8    VND0009                       Romero LLC         Maintenance   \n",
      "9    VND0010                 GREGORY AND SONS          Consulting   \n",
      "10   VND0011                     Guerrero PLC         IT Services   \n",
      "11   VND0012  Gonzalez, Stewart and Zimmerman            Supplies   \n",
      "12   VND0013                   Fuller-Ramirez  Financial Services   \n",
      "13   VND0014                        Lee-Smith         Maintenance   \n",
      "14      None                    Ochoa-Hoffman                None   \n",
      "15   VND0016     whitaker, ramirez and gordon         Maintenance   \n",
      "16   VND0017                       Morrow Ltd  Financial Services   \n",
      "17   VND0018                    Hernandez PLC            Supplies   \n",
      "18   VND0019                     Burke-Vargas         IT Services   \n",
      "19   VND0020                  Alexander Group  Financial Services   \n",
      "\n",
      "   Contract_Start_Date          Contract_End_Date  Amount_Paid_YTD  \n",
      "0           2023-10-25 2025-09-30 00:00:00.000000     8.948449e+04  \n",
      "1           2021-10-15 2025-02-01 00:00:00.000000     7.573647e+04  \n",
      "2           2020-09-01 2025-04-17 15:42:22.835525     4.444781e+04  \n",
      "3           2022-02-21 2024-11-04 00:00:00.000000     1.602284e+04  \n",
      "4           2022-03-19 2025-04-17 15:42:22.835525     8.865740e+04  \n",
      "5           2022-07-03 2024-07-03 00:00:00.000000     3.182959e+04  \n",
      "6           2021-11-14 2025-01-24 00:00:00.000000     1.592402e+05  \n",
      "7           2024-10-21 2027-08-18 00:00:00.000000     1.348047e+05  \n",
      "8           2021-07-02 2025-04-17 15:42:22.835525     1.939453e+05  \n",
      "9           2021-05-26                        NaT     2.508643e+02  \n",
      "10          2024-12-25 2026-01-30 00:00:00.000000     1.939018e+05  \n",
      "11          2022-09-14 2025-07-05 00:00:00.000000     8.527902e+04  \n",
      "12          2022-02-28 2024-08-09 00:00:00.000000     1.787812e+05  \n",
      "13          2021-05-16 2025-04-17 15:42:22.835525     1.503632e+05  \n",
      "14          2023-06-29 2025-04-17 15:42:22.835525     1.838942e+05  \n",
      "15          2023-01-23                        NaT     1.108611e+07  \n",
      "16          2021-10-06 2025-04-17 15:42:22.835525     1.001502e+05  \n",
      "17          2020-07-22 2025-04-17 15:42:22.835525     5.012740e+03  \n",
      "18          2024-05-28 2026-04-02 00:00:00.000000     1.858419e+05  \n",
      "19          2023-06-03 2025-10-24 00:00:00.000000     1.640661e+05  \n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Introduce Duplicate Rows\n",
    "num_duplicates = int(len(external_vendor_df) * 0.02)  # 2% duplicates\n",
    "duplicates_df = external_vendor_df.sample(n=num_duplicates, random_state=42)\n",
    "external_vendor_df = pd.concat([external_vendor_df, duplicates_df], ignore_index=True)\n",
    "\n",
    "# Introduce Inconsistent Data Formats\n",
    "# Convert some dates to strings\n",
    "num_inconsistent_dates = int(len(external_vendor_df) * 0.05)  # 5% inconsistent dates\n",
    "date_format_indices = external_vendor_df.sample(n=num_inconsistent_dates, random_state=42).index\n",
    "external_vendor_df.loc[date_format_indices, 'Contract_Start_Date'] = external_vendor_df.loc[\n",
    "    date_format_indices, 'Contract_Start_Date'\n",
    "].astype(str)\n",
    "\n",
    "# Modify Vendor Names to Introduce Inconsistencies\n",
    "vendor_name_indices = external_vendor_df.sample(n=num_inconsistent_dates, random_state=42).index\n",
    "external_vendor_df.loc[vendor_name_indices, 'Vendor_Name'] = external_vendor_df.loc[\n",
    "    vendor_name_indices, 'Vendor_Name'\n",
    "].apply(lambda x: x.lower() if random.random() > 0.5 else x.upper())\n",
    "\n",
    "# Introduce Outliers in Amount_Paid_YTD\n",
    "outlier_indices = external_vendor_df.sample(n=num_inconsistent_dates, random_state=42).index\n",
    "external_vendor_df.loc[outlier_indices, 'Amount_Paid_YTD'] = external_vendor_df.loc[\n",
    "    outlier_indices, 'Amount_Paid_YTD'\n",
    "] * np.random.choice([0.01, 100], size=num_inconsistent_dates)\n",
    "\n",
    "# Introduce Logical Inconsistencies\n",
    "# Set Contract_End_Date earlier than Contract_Start_Date\n",
    "logical_error_indices = external_vendor_df.sample(n=num_inconsistent_dates, random_state=42).index\n",
    "external_vendor_df.loc[logical_error_indices, 'Contract_End_Date'] = external_vendor_df.loc[\n",
    "    logical_error_indices, 'Contract_Start_Date'\n",
    "] - pd.Timedelta(days=random.randint(1, 365))\n",
    "\n",
    "# Introduce Missing Values (Nulls) - LAST STEP\n",
    "num_nulls = int(len(external_vendor_df) * 0.05)  # 5% nulls\n",
    "null_indices_end_date = external_vendor_df.sample(n=num_nulls, random_state=42).index\n",
    "null_indices_service_type = external_vendor_df.sample(n=num_nulls, random_state=24).index\n",
    "null_indices_vendor_id = external_vendor_df.sample(n=num_nulls, random_state=36).index\n",
    "\n",
    "external_vendor_df.loc[null_indices_end_date, 'Contract_End_Date'] = pd.NaT\n",
    "external_vendor_df.loc[null_indices_service_type, 'Service_Type'] = None\n",
    "external_vendor_df.loc[null_indices_vendor_id, 'Vendor_ID'] = None\n",
    "\n",
    "# Preview the table with errors\n",
    "print(external_vendor_df.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Save the table to a CSV file\n",
    "external_vendor_df.to_csv('external_vendor_information.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Dataset #4: Unclaimed Property Records\n",
    "Purpose: Evaluate effectiveness of ISTO's unclaimed property program, identify trends, and visualize property types.\n",
    "Table Type: Fact Table\n",
    "**Schema:** \n",
    "- Property_ID VARCHAR\n",
    "- Owner_Name VARCHAR\n",
    "- Property_Type VARCHAR\n",
    "- Reported_Date DATETIME\n",
    "- Property_Value FLOAT\n",
    "- Claim_Status VARCHAR\n",
    "\n",
    "Considerations: \n",
    "1. There will be a mix of vendors and people who own unclaimed property\n",
    "2. Some of the vendors will be active vendors, but some will not be active, and will not appear in any of the tables (perhaps former contract holders)\n",
    "3. Dates will only be investigated quarterly; this isn't a daily investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Property_ID  Owner_Type                     Owner_Name     Property_Type  \\\n",
      "0      UP0001      Vendor  Rodriguez, Nguyen and Elliott  Safe Deposit Box   \n",
      "1      UP0002  Individual                  Cynthia Baker      Bank Account   \n",
      "2      UP0003  Individual                  Andrew Barnes   Insurance Claim   \n",
      "3      UP0004  Individual                Jennifer Briggs  Safe Deposit Box   \n",
      "4      UP0005  Individual                  Krista Spence   Insurance Claim   \n",
      "\n",
      "  Reported_Date  Property_Value Claim_Status  \n",
      "0    2018-09-30         5011.73    Unclaimed  \n",
      "1    2022-03-31         8860.79    Unclaimed  \n",
      "2    2022-03-31         6253.99    Unclaimed  \n",
      "3    2018-12-31         7916.36    Unclaimed  \n",
      "4    2018-12-31          443.81    Unclaimed  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\justi\\AppData\\Local\\Temp\\ipykernel_4372\\3275233515.py:36: FutureWarning: 'Q' is deprecated and will be removed in a future version, please use 'QE' instead.\n",
      "  quarters = pd.date_range(start='2018-01-01', end='2023-12-31', freq='Q')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from faker import Faker\n",
    "\n",
    "# Initialize Faker for generating random data\n",
    "fake = Faker()\n",
    "np.random.seed(91942)\n",
    "\n",
    "# Number of unclaimed property records\n",
    "num_records = 200\n",
    "\n",
    "# Use active vendors from the Transactions Table\n",
    "# unique_vendors = unique_vendors # from the Transactions processing\n",
    "active_vendors = np.random.choice(unique_vendors, size=int(num_records * 0.1), replace=False)  # 40% active vendors\n",
    "\n",
    "# Generate new vendors (inactive vendors)\n",
    "inactive_vendors = [fake.company() for _ in range(int(num_records * 0.4))]  # 30% inactive vendors\n",
    "\n",
    "# Generate individuals \n",
    "individuals = [fake.name() for _ in range(int(num_records * 0.5))]\n",
    "\n",
    "# Combine active and inactive vendors\n",
    "owner_names = np.concatenate([active_vendors, inactive_vendors, individuals])\n",
    "np.random.shuffle(owner_names)  # Shuffle to mix active and inactive vendors\n",
    "\n",
    "# Generate random Owner Types (Individual or Vendor)\n",
    "owner_types = ['Vendor' if owner in active_vendors or owner in inactive_vendors else \"Individual\" for owner in owner_names]\n",
    "\n",
    "# Generate random Property Types\n",
    "property_types = np.random.choice(\n",
    "    ['Bank Account', 'Insurance Claim', 'Stocks', 'Safe Deposit Box', 'Uncashed Check', 'Bonds'],\n",
    "    size=num_records\n",
    ")\n",
    "\n",
    "# Generate random Reported Dates (limited to quarterly intervals in the past 5 years)\n",
    "quarters = pd.date_range(start='2018-01-01', end='2023-12-31', freq='Q')\n",
    "reported_dates = np.random.choice(quarters, size=num_records)\n",
    "\n",
    "# Generate random Property Values\n",
    "property_values = np.round(np.random.uniform(100, 10000, size=num_records), 2)\n",
    "\n",
    "# Generate random Claim Status\n",
    "claim_status = np.random.choice(['Claimed', 'Unclaimed'], size=num_records, p=[0.3, 0.7])\n",
    "\n",
    "# Create the Unclaimed Property Records DataFrame\n",
    "unclaimed_property_df = pd.DataFrame({\n",
    "    'Property_ID': [f'UP{str(i).zfill(4)}' for i in range(1, num_records + 1)],\n",
    "    'Owner_Type': owner_types,\n",
    "    'Owner_Name': owner_names,\n",
    "    'Property_Type': property_types,\n",
    "    'Reported_Date': reported_dates,\n",
    "    'Property_Value': property_values,\n",
    "    'Claim_Status': claim_status\n",
    "})\n",
    "\n",
    "# Preview the table\n",
    "print(unclaimed_property_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oooooh, nice data again. Let's rain on this parade with some duplicates, inconsistencies, outliers and nulls! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Property_ID  Owner_Type                     Owner_Name     Property_Type  \\\n",
      "0       UP0001      Vendor  Rodriguez, Nguyen and Elliott  Safe Deposit Box   \n",
      "1       UP0002  Individual                  Cynthia Baker      Bank Account   \n",
      "2       UP0003  Individual                  Andrew Barnes   Insurance Claim   \n",
      "3       UP0004  Individual                Jennifer Briggs  Safe Deposit Box   \n",
      "4       UP0005  Individual                  Krista Spence   Insurance Claim   \n",
      "5       UP0006      Vendor                  Johnson Group   Insurance Claim   \n",
      "6       UP0007      Vendor                   Burke-Vargas    Uncashed Check   \n",
      "7       UP0008  Individual                  David Calhoun    Uncashed Check   \n",
      "8       UP0009      Vendor                      White Ltd             Bonds   \n",
      "9       UP0010      Vendor                  Maldonado LLC    Uncashed Check   \n",
      "10      UP0011  Individual                   Kylie Larsen    Uncashed Check   \n",
      "11      UP0012      Vendor                 Watts-Erickson            Stocks   \n",
      "12      UP0013  Individual               Jessica Williams    Uncashed Check   \n",
      "13      UP0014      Vendor                     French PLC             Bonds   \n",
      "14      UP0015      Vendor               Peterson-Johnson  Safe Deposit Box   \n",
      "15      UP0016      Vendor                   Brown-Lucero             Bonds   \n",
      "16      UP0017  Individual                  Eric Martinez   Insurance Claim   \n",
      "17      UP0018      Vendor        Miller, Singh and Smith   Insurance Claim   \n",
      "18      UP0019      Vendor                           None             Bonds   \n",
      "19      UP0020  Individual                Susan Robertson            Stocks   \n",
      "\n",
      "   Reported_Date  Property_Value Claim_Status  \n",
      "0     2018-09-30       5011.7300    Unclaimed  \n",
      "1     2022-03-31       8860.7900    Unclaimed  \n",
      "2     2022-03-31       6253.9900    Unclaimed  \n",
      "3     2018-12-31       7916.3600    Unclaimed  \n",
      "4     2018-12-31        443.8100    Unclaimed  \n",
      "5     2022-09-30        195.3900      Claimed  \n",
      "6     2022-03-31       1105.7600    Unclaimed  \n",
      "7     2023-03-31       5789.1600    Unclaimed  \n",
      "8     2023-03-31       6484.3900      Claimed  \n",
      "9     2022-09-30       6958.5200    Unclaimed  \n",
      "10    2020-06-30       3519.3100    Unclaimed  \n",
      "11    2023-06-30       7142.3300    Unclaimed  \n",
      "12    2022-12-31       9611.3100    Unclaimed  \n",
      "13    2023-12-31        588.1200    Unclaimed  \n",
      "14    2022-03-31       4853.3400    Unclaimed  \n",
      "15    2022-03-31       3563.7000      Claimed  \n",
      "16    2022-09-30       3734.6900    Unclaimed  \n",
      "17    2019-03-31         25.8937    Unclaimed  \n",
      "18    2023-06-30       6835.2300    Unclaimed  \n",
      "19    2023-06-30       3138.8700      Claimed  \n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Introduce Duplicate Records\n",
    "num_duplicates = int(len(unclaimed_property_df) * 0.02)  # 2% duplicates\n",
    "duplicates_df = unclaimed_property_df.sample(n=num_duplicates, random_state=random_state)\n",
    "unclaimed_property_df = pd.concat([unclaimed_property_df, duplicates_df], ignore_index=True)\n",
    "\n",
    "# Introduce Inconsistent Formats\n",
    "# Convert some dates to strings\n",
    "num_inconsistent_dates = int(len(unclaimed_property_df) * 0.01)  # 1% inconsistent dates\n",
    "date_format_indices = unclaimed_property_df.sample(n=num_inconsistent_dates, random_state=random_state).index\n",
    "unclaimed_property_df.loc[date_format_indices, 'Reported_Date'] = unclaimed_property_df.loc[\n",
    "    date_format_indices, 'Reported_Date'\n",
    "].astype(str)\n",
    "\n",
    "# Modify Owner Names to Introduce Inconsistencies\n",
    "owner_name_indices = unclaimed_property_df.sample(n=num_inconsistent_dates, random_state=random_state).index\n",
    "unclaimed_property_df.loc[owner_name_indices, 'Owner_Name'] = unclaimed_property_df.loc[\n",
    "    owner_name_indices, 'Owner_Name'\n",
    "].apply(lambda x: x.lower() if isinstance(x, str) and random.random() > 0.5 else x.upper() if isinstance(x, str) else x)\n",
    "\n",
    "# Introduce Logical Inconsistencies\n",
    "# Set Claim_Status to \"Claimed\" but leave Owner_Name as null\n",
    "logical_error_indices = unclaimed_property_df.sample(n=num_inconsistent_dates, random_state=random_state).index\n",
    "unclaimed_property_df.loc[logical_error_indices, 'Claim_Status'] = 'Claimed'\n",
    "unclaimed_property_df.loc[logical_error_indices, 'Owner_Name'] = None\n",
    "\n",
    "# Introduce Outliers in Property_Value\n",
    "outlier_indices = unclaimed_property_df.sample(n=num_inconsistent_dates, random_state=random_state).index\n",
    "unclaimed_property_df.loc[outlier_indices, 'Property_Value'] = unclaimed_property_df.loc[\n",
    "    outlier_indices, 'Property_Value'\n",
    "] * np.random.choice([0.01, 1000], size=num_inconsistent_dates)\n",
    "\n",
    "# Introduce Missing Values (Nulls) - LAST STEP\n",
    "num_nulls = int(len(unclaimed_property_df) * 0.01)  # 1% nulls\n",
    "null_indices_owner_name = unclaimed_property_df.sample(n=num_nulls, random_state=random_state).index\n",
    "null_indices_property_type = unclaimed_property_df.sample(n=num_nulls, random_state=random_state_alt).index\n",
    "\n",
    "unclaimed_property_df.loc[null_indices_owner_name, 'Owner_Name'] = None\n",
    "unclaimed_property_df.loc[null_indices_property_type, 'Property_Type'] = None\n",
    "\n",
    "# Preview the table with errors\n",
    "print(unclaimed_property_df.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Optional: Save the table to a CSV file\n",
    "unclaimed_property_df.to_csv('unclaimed_property_records.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Dataset 5: Program Performance Metrics\n",
    "Purpose: Evaluate effectiveness and impact of ISTO programs, identify areas for improvement.\n",
    "\n",
    "Table Type: Dimension\n",
    "\n",
    "**Schema:**\n",
    "- Program_ID VARCHAR\n",
    "- Program_Name VARCHAR\n",
    "- Reporting_Period DATETIME\n",
    "- Participants VARCHAR\n",
    "- Successful_Outcomes VARCHAR\n",
    "- Program_Cost FLOAT\n",
    "\n",
    "Considerations: \n",
    "This table aggregates some of the data from other existing tables, so it cannot be generated without using some of the previous data, while interpolating additional information. \n",
    "1. Program Cost aggregation from Transactions\n",
    "2. Participant-related transaction count\n",
    "3. Vendor Costs from External Vendor Information\n",
    "4. Unclaimed property such as refunds or grants not climaed\n",
    "5. Investment Portfolio Holding gunding sources that can link to market value or quantity held\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Reporting_Period        Program_Name  Program_Cost  Participants  \\\n",
      "0       2023-06-30     Consulting Fees    1038370.83           445   \n",
      "1       2023-06-30   Event Sponsorship    1236303.87           692   \n",
      "2       2023-06-30       Grant Funding     600633.30           243   \n",
      "3       2023-06-30     Interest Income     629410.32           329   \n",
      "4       2023-06-30  Investment Returns     653405.84           910   \n",
      "\n",
      "   Successful_Outcomes Program_ID  Budget_Allocation  Budget_Utilization_Rate  \\\n",
      "0                  268     PRG001         1231197.25                    84.34   \n",
      "1                  549     PRG002         1740815.19                    71.02   \n",
      "2                  171     PRG003          769051.49                    78.10   \n",
      "3                  187     PRG004          760824.21                    82.73   \n",
      "4                  561     PRG005          910702.12                    71.75   \n",
      "\n",
      "  Completion_Rate  Participant_Satisfaction  Cost_per_Participant  \\\n",
      "0          60.22%                       3.8               2333.42   \n",
      "1          79.34%                       4.6               1786.57   \n",
      "2          70.37%                       3.5               2471.74   \n",
      "3          56.84%                       3.5               1913.10   \n",
      "4          61.65%                       3.6                718.03   \n",
      "\n",
      "   Cost_per_Successful_Outcome On_Time_Completion  \n",
      "0                      3874.52                Yes  \n",
      "1                      2251.92                 No  \n",
      "2                      3512.48                 No  \n",
      "3                      3365.83                 No  \n",
      "4                      1164.72                Yes  \n"
     ]
    }
   ],
   "source": [
    "# Ensure Transaction_Date is in datetime format\n",
    "transactions_clean_df['Transaction_Date'] = pd.to_datetime(transactions_clean_df['Transaction_Date'], errors='coerce')\n",
    "\n",
    "# Drop rows with invalid or missing Transaction_Date\n",
    "transactions_clean_df = transactions_clean_df.dropna(subset=['Transaction_Date'])\n",
    "\n",
    "# Filter transactions related to programs\n",
    "program_related_transactions = transactions_clean_df[\n",
    "    (transactions_clean_df['Description'] == 'Program Fees') |\n",
    "    (transactions_clean_df['Department'] == 'Community Programs')\n",
    "]\n",
    "\n",
    "# Aggregate program costs by reporting period (quarterly)\n",
    "program_metrics = program_related_transactions.groupby(\n",
    "    [pd.Grouper(key='Transaction_Date', freq='QE'), 'Description']\n",
    ").agg(\n",
    "    Program_Cost=('Transaction_Amount', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Rename columns for clarity\n",
    "program_metrics.rename(columns={'Description': 'Program_Name', 'Transaction_Date': 'Reporting_Period'}, inplace=True)\n",
    "\n",
    "# Generate random participants for each program\n",
    "program_metrics['Participants'] = np.random.randint(50, 1000, size=len(program_metrics))\n",
    "\n",
    "# Generate successful outcomes as a subset of participants\n",
    "program_metrics['Successful_Outcomes'] = program_metrics['Participants'].apply(lambda x: np.random.randint(int(x * 0.5), x + 1))\n",
    "\n",
    "# Add unique Program IDs\n",
    "program_metrics['Program_ID'] = [f'PRG{str(i).zfill(3)}' for i in range(1, len(program_metrics) + 1)]\n",
    "\n",
    "# Add additional metrics\n",
    "\n",
    "# Example budget allocations (randomized for demonstration purposes)\n",
    "program_metrics['Budget_Allocation'] = np.round(program_metrics['Program_Cost'] * np.random.uniform(1.1, 1.5, size=len(program_metrics)), 2)\n",
    "\n",
    "# Calculate Budget Utilization Rate\n",
    "program_metrics['Budget_Utilization_Rate'] = (program_metrics['Program_Cost'] / program_metrics['Budget_Allocation'] * 100).round(2)\n",
    "\n",
    "# Calculate Completion Rate\n",
    "program_metrics['Completion_Rate'] = (program_metrics['Successful_Outcomes'] / program_metrics['Participants'] * 100).round(2).astype(str) + '%'\n",
    "\n",
    "# Generate Participant Satisfaction scores (randomized between 3.5 and 5.0)\n",
    "program_metrics['Participant_Satisfaction'] = [round(np.random.uniform(3.5, 5.0), 1) for _ in range(len(program_metrics))]\n",
    "\n",
    "# Calculate Cost per Participant\n",
    "program_metrics['Cost_per_Participant'] = (program_metrics['Program_Cost'] / program_metrics['Participants']).round(2)\n",
    "\n",
    "# Calculate Cost per Successful Outcome\n",
    "program_metrics['Cost_per_Successful_Outcome'] = (program_metrics['Program_Cost'] / program_metrics['Successful_Outcomes']).round(2)\n",
    "\n",
    "# Add On-Time Completion status (randomized for demonstration purposes)\n",
    "program_metrics['On_Time_Completion'] = np.random.choice(['Yes', 'No'], size=len(program_metrics))\n",
    "\n",
    "# Preview the updated Program Performance Metrics Table\n",
    "print(program_metrics.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lovely. Guess what we're going to do with this masterpiece? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Reporting_Period           Program_Name Program_Cost  Participants  \\\n",
      "0   2025-06-30 00:00:00     Investment Returns    113747.08         588.0   \n",
      "1   2024-09-30 00:00:00        Office Supplies   2665792.66         884.0   \n",
      "2   2023-12-31 00:00:00  Software Subscription   1753907.94         920.0   \n",
      "3   2024-09-30 00:00:00     Investment Returns    830233.31         583.0   \n",
      "4   2024-12-31 00:00:00        Travel Expenses   2466356.86         704.0   \n",
      "5   2024-12-31 00:00:00  Software Subscription   1484993.45         878.0   \n",
      "6    2024-12-31 INVALID     Investment Returns   1228261.45         783.0   \n",
      "7   2024-03-31 00:00:00        Interest Income     725662.6         514.0   \n",
      "8   2023-12-31 00:00:00            Maintenance   2186279.41         546.0   \n",
      "9   2025-06-30 00:00:00        Travel Expenses    952662.71         591.0   \n",
      "10  2025-06-30 00:00:00  Software Subscription    333186.88         231.0   \n",
      "11  2024-03-31 00:00:00      Event Sponsorship   1952318.22         966.0   \n",
      "12  2024-09-30 00:00:00        Interest Income   1279843.88         302.0   \n",
      "13  2025-03-31 00:00:00        Interest Income    765392.81         331.0   \n",
      "14  2024-06-30 00:00:00        Travel Expenses   1201519.36         966.0   \n",
      "15  2024-03-31 00:00:00            Maintenance   1538246.93         683.0   \n",
      "16  2024-06-30 00:00:00          Grant Funding   1086811.02         485.0   \n",
      "17  2023-09-30 00:00:00        Consulting Fees   2090448.28           NaN   \n",
      "18  2023-09-30 00:00:00           Program Fees   4363156.86         975.0   \n",
      "19  2025-06-30 00:00:00        Consulting Fees    129071.41         486.0   \n",
      "\n",
      "    Successful_Outcomes Program_ID  Budget_Allocation  \\\n",
      "0                   337     PRG093          141654.88   \n",
      "1                   458     PRG062         3930611.72   \n",
      "2                   518     PRG032         2360736.03   \n",
      "3                   529     PRG060         1094610.40   \n",
      "4                   368     PRG077         3126629.70   \n",
      "5                   780     PRG076         2155081.23   \n",
      "6                   762     PRG071         1820190.24   \n",
      "7                   270     PRG037          984401.64   \n",
      "8                   371     PRG028                NaN   \n",
      "9                   460     PRG097         1286984.11   \n",
      "10                  211     PRG096          481495.99   \n",
      "11                  925     PRG035         2254009.33   \n",
      "12                  160     PRG059         1900365.45   \n",
      "13                  370     PRG081         1089673.42   \n",
      "14                  809     PRG055         1356675.27   \n",
      "15                  572     PRG039         1986818.16   \n",
      "16                  312     PRG047         1592458.07   \n",
      "17                  609     PRG012         2565762.42   \n",
      "18                  620     PRG019         4988137.53   \n",
      "19                  280     PRG089          148790.52   \n",
      "\n",
      "    Budget_Utilization_Rate Completion_Rate  Participant_Satisfaction  \\\n",
      "0                     80.30          57.31%                       4.6   \n",
      "1                     67.82          51.81%                       4.8   \n",
      "2                     74.29           56.3%                       3.9   \n",
      "3                     75.85          90.74%                       3.9   \n",
      "4                     78.88          52.27%                       4.5   \n",
      "5                     68.91          88.84%                       3.9   \n",
      "6                     67.48          97.32%                       4.3   \n",
      "7                     73.72          52.53%                       4.0   \n",
      "8                     72.27          67.95%                       4.9   \n",
      "9                     74.02          77.83%                       4.0   \n",
      "10                    69.20          91.34%                       3.6   \n",
      "11                    86.62          95.76%                       4.4   \n",
      "12                    67.35          52.98%                       3.7   \n",
      "13                    70.24          89.43%                       4.6   \n",
      "14                    88.56          83.75%                       3.7   \n",
      "15                    77.42          83.75%                       4.7   \n",
      "16                    68.25          64.33%                       4.0   \n",
      "17                    81.47          91.03%                       4.4   \n",
      "18                    87.47          63.59%                       4.0   \n",
      "19                    86.75          57.61%                       4.2   \n",
      "\n",
      "    Cost_per_Participant  Cost_per_Successful_Outcome On_Time_Completion  \n",
      "0                 193.45                       337.53                 No  \n",
      "1                3015.60                      5820.51                 No  \n",
      "2                1906.42                      3385.92                 No  \n",
      "3                1424.07                      1569.44                 No  \n",
      "4                3503.35                      6702.06                Yes  \n",
      "5                1691.34                      1903.84                 No  \n",
      "6                1568.66                      1611.89                 No  \n",
      "7                1411.79                      2687.64                Yes  \n",
      "8                4004.17                      5892.94                Yes  \n",
      "9                1611.95                      2071.01                Yes  \n",
      "10               1442.37                      1579.08                Yes  \n",
      "11               2021.03                      2110.61                 No  \n",
      "12               4237.89                      7999.02                Yes  \n",
      "13               2312.36                      2585.79                Yes  \n",
      "14               1243.81                      1485.19                Yes  \n",
      "15               2252.19                      2689.24                 No  \n",
      "16               2240.85                      3483.37                 No  \n",
      "17               3124.74                      3432.59                Yes  \n",
      "18               4475.03                      7037.35                 No  \n",
      "19                265.58                       460.97                Yes  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\justi\\AppData\\Local\\Temp\\ipykernel_4372\\2675814484.py:35: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['2024-12-31 INVALID' '2025-03-31 INVALID']' has dtype incompatible with datetime64[ns], please explicitly cast to a compatible dtype first.\n",
      "  program_metrics.loc[format_error_indices, 'Reporting_Period'] = program_metrics.loc[\n",
      "C:\\Users\\justi\\AppData\\Local\\Temp\\ipykernel_4372\\2675814484.py:48: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['1252925.62' '4363156.86']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  program_metrics.loc[type_error_indices, 'Program_Cost'] = program_metrics.loc[\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# 1. Introduce Logical Inconsistencies\n",
    "# Set Successful_Outcomes greater than Participants\n",
    "logical_error_indices = program_metrics.sample(n=num_nulls, random_state=random_state).index\n",
    "program_metrics.loc[logical_error_indices, 'Successful_Outcomes'] = program_metrics.loc[\n",
    "    logical_error_indices, 'Participants'\n",
    "].apply(lambda x: x + random.randint(1, 50) if x is not None else None)\n",
    "\n",
    "# Set Budget_Utilization_Rate above 100% or below 0%\n",
    "budget_error_indices = program_metrics.sample(n=num_nulls, random_state=random_state_alt).index\n",
    "program_metrics.loc[budget_error_indices, 'Budget_Utilization_Rate'] = program_metrics.loc[\n",
    "    budget_error_indices, 'Budget_Utilization_Rate'\n",
    "].apply(lambda x: x * random.choice([-1, 2]))\n",
    "\n",
    "# Assign On_Time_Completion as \"Yes\" for programs with no participants\n",
    "completion_error_indices = program_metrics[program_metrics['Participants'].isnull()].index\n",
    "program_metrics.loc[completion_error_indices, 'On_Time_Completion'] = 'Yes'\n",
    "\n",
    "# 2. Introduce Outliers\n",
    "# Add extreme values to Program_Cost\n",
    "outlier_indices = program_metrics.sample(n=num_nulls, random_state=random_state).index\n",
    "program_metrics.loc[outlier_indices, 'Program_Cost'] = program_metrics.loc[\n",
    "    outlier_indices, 'Program_Cost'\n",
    "] * random.choice([10, 0.01])\n",
    "\n",
    "# 3. Introduce Duplicate Rows\n",
    "num_duplicates = int(len(program_metrics) * 0.02)  # 2% duplicates\n",
    "duplicates_df = program_metrics.sample(n=num_duplicates, random_state=random_state)\n",
    "program_metrics = pd.concat([program_metrics, duplicates_df], ignore_index=True)\n",
    "\n",
    "# 4. Introduce Inconsistent Formats\n",
    "# Convert some Reporting_Period values to strings\n",
    "format_error_indices = program_metrics.sample(n=num_nulls, random_state=random_state).index\n",
    "program_metrics.loc[format_error_indices, 'Reporting_Period'] = program_metrics.loc[\n",
    "    format_error_indices, 'Reporting_Period'\n",
    "].astype(str) + ' INVALID'\n",
    "\n",
    "# Modify Program_Name to introduce inconsistent capitalization\n",
    "name_error_indices = program_metrics.sample(n=num_nulls, random_state=random_state).index\n",
    "program_metrics.loc[name_error_indices, 'Program_Name'] = program_metrics.loc[\n",
    "    name_error_indices, 'Program_Name'\n",
    "].apply(lambda x: x.lower() if random.random() > 0.5 else x.upper())\n",
    "\n",
    "# 5. Introduce Incorrect Data Types\n",
    "# Convert numeric columns to strings\n",
    "type_error_indices = program_metrics.sample(n=num_nulls, random_state=random_state).index\n",
    "program_metrics.loc[type_error_indices, 'Program_Cost'] = program_metrics.loc[\n",
    "    type_error_indices, 'Program_Cost'\n",
    "].astype(str)\n",
    "\n",
    "# 6. Introduce Missing Values (Nulls)\n",
    "num_nulls = int(len(program_metrics) * 0.05)  # 5% nulls\n",
    "null_indices_participants = program_metrics.sample(n=num_nulls, random_state=random_state).index\n",
    "null_indices_budget = program_metrics.sample(n=num_nulls, random_state=random_state_alt).index\n",
    "\n",
    "program_metrics.loc[null_indices_participants, 'Participants'] = None\n",
    "program_metrics.loc[null_indices_budget, 'Budget_Allocation'] = None\n",
    "\n",
    "# Shuffle the DataFrame to mix errors naturally\n",
    "program_metrics = program_metrics.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "\n",
    "# Preview the table with errors\n",
    "print(program_metrics.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Program Performance Metrics Table has been mutilated and regurgitated as 'program_performance_metrics.xlsx'.\n"
     ]
    }
   ],
   "source": [
    "# Save to excel, frown, cry... just let it all out.\n",
    "program_metrics.to_excel('program_performance_metrics.xlsx', index=False, sheet_name='Program Metrics')\n",
    "\n",
    "# Confirm the file was saved in such a horrid manner\n",
    "print(\"Program Performance Metrics Table has been mutilated and regurgitated as 'program_performance_metrics.xlsx'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, alright, alright! \n",
    "The last step is to just verify that we have all of these tables saved in the crap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying External Vendor Information (external_vendor_information.csv):\n",
      "This crappy table exists. Lucky you. \n",
      "\n",
      "Verifying Portfolio Holdings (portfolio_holdings.csv):\n",
      "This crappy table exists. Lucky you. \n",
      "\n",
      "Verifying Transactions (transactions.csv):\n",
      "This crappy table exists. Lucky you. \n",
      "\n",
      "Verifying Unclaimed Property Records (unclaimed_property_records.csv):\n",
      "This crappy table exists. Lucky you. \n",
      "\n",
      "Verifying Program Performance Metrics (program_performance_metrics.xlsx):\n",
      "This crappy table exists. Lucky you. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File paths\n",
    "files = {\n",
    "    \"External Vendor Information\": \"external_vendor_information.csv\"\n",
    "    , \"Portfolio Holdings\": \"portfolio_holdings.csv\"\n",
    "    , \"Transactions\": \"transactions.csv\"\n",
    "    , \"Unclaimed Property Records\": \"unclaimed_property_records.csv\"\n",
    "    , \"Program Performance Metrics\": \"program_performance_metrics.xlsx\"\n",
    "}\n",
    "\n",
    "# Loop through each file, read it, and print the head\n",
    "for table_name, file_path in files.items():\n",
    "    print(f\"Verifying {table_name} ({file_path}):\")\n",
    "    try:\n",
    "        # Check file extension to determine how to read the file\n",
    "        if file_path.endswith('.csv'):\n",
    "            df = pd.read_csv(file_path)\n",
    "        elif file_path.endswith('.xlsx'):\n",
    "            df = pd.read_excel(file_path)\n",
    "        else:\n",
    "            print(f\"Unsupported file format for {file_path}. Skipping...\\n\")\n",
    "            continue\n",
    "        \n",
    "        # Print the first few rows of the DataFrame\n",
    "        if not df.empty:\n",
    "            print(\"This crappy table exists. Lucky you.\", \"\\n\")\n",
    "        else:\n",
    "            print(f\"The file {file_path} is empty.\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error was squeezed out while reading {file_path}: {e}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "treasury",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
